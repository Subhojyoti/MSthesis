In this thesis, we studied three complex bandit problems, the stochastic multi-armed bandit (SMAB) with the goal of cumulative regret minimization, pure exploration stochastic thresholding bandit problem (TBP) with the goal of expected loss minimization and piecewise stationary bandits with the goal of finding adaptive algorithms which detect and adapts to changepoints. For the first problem, we devised a novel algorithm called Efficient UCB Variance (EUCBV) which enjoys an order optimal regret bound and performs superbly in diverse stochastic environments. In the second part, the thresholding bandit problem, we came up with the novel algorithm called Augmented UCB (AugUCB) which is the first algorithm to use variance estimation for the considered TBP setting and also empirically outperforms most of the other algorithms. Finally, we studied the piecewise stochastic bandits and proposed several solutions for adaptive algorithms that detect changepoints and try to adapt accordingly.

    There are several directions in which the work done in this thesis can be extended. Starting with the SMAB setting, there are many fundamental questions that need to be answered. Though EUCBV reached an order optimal regret bound of $80\sqrt{KT}$, still the constant associated with the bound is quite large and can be reduced by finer analysis. One avenue for future work is to remove the constraint of $T\geq K^{2.4}$ required for EUCBV to reach the order optimal regret bound. Also, EUCBV does not have any asymptotic guarantee and we do not know whether it can reach the \citet{lai1985asymptotically} asymptotic lower bound discussed in chapter \ref{chap:SMAB}. Recently another algorithm called KL-UCB++ \citep{menard2017minimax} has been proved to be both minimax optimal and asymptotically optimal. Further, EUCBV requires the knowledge of horizon as input and it will be interesting to find an anytime version of EUCBV. Similar anytime version of MOSS \citep{degenne2016anytime} and OCUCB \citep{lattimore2016regret} has also been proposed in literature.
    
    The thresholding bandit problem is also being intensely studied in the bandit community and there are several directions where this work can be extended. One way is to modify the APT algorithm itself and come up with a variance adaptive version of APT. This has bee recently studied in \citet{kano2017good}. Also, currently there are no lower bounds for the TBP setting considering only variance estimation and it will be interesting to derive a lower bound for this setting. Again, whereas APT is an anytime algorithm AugUCB is not anytime and it needs several modifications to obtain an anytime version of AugUCB. Further, we can also derive a gap-independent and gap-dependent bounds for AugUCB as like APT. In \citet{lattimore2015optimally} the authors showed that APT like UCB1 enjoys a gap-dependent cumulative regret bound of $O\left( \dfrac{K\log T}{\Delta^2}\right)$ and gap-independent regret bound of $O\left( \sqrt{KT\log T}\right)$.
    
    For the piecewise stochastic bandits we were able to show empirically that the adaptive algorithms we considered are indeed more powerful than passive algorithms and further work needs to be done to derive the regret bounds for these algorithms. 
    %Specifically for CPD\ref{alg:CPD3} we find the confidence interval obtained by Laplace method as like UCB-$\delta$ algorithm\citep{abbasi2011improved} very promising and more work needs to be done in this aspect.
    
    Finally, to summarize everything, the bandit community is actively researching several of these open problems discussed here and we hope to answer some of these problems in near future. Further, several interesting variations of the problems discussed here are also being studied such as the Contextual Thresholding Bandit problem, Combinatorial Bandit problems \citep{cesa2012combinatorial} and more powerful algorithms for changepoint detection.

