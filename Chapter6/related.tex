Multi-armed bandits (MAB) have been extensively studied in the \textit{stochastic setting} where the distribution associated with each arm is fixed throughout the time horizon. This setting has been extensively discussed in chapter~\ref{chap:SMAB}. Starting from the seminal works of \citet{thompson1933likelihood}, \citet{robbins1952some} and finally in \citet{lai1985asymptotically} the authors provide the asymptotic lower bound for the class of algorithms considered.  The upper confidence bound (UCB) algorithms, which are a type of index-based frequentist strategy, were first proposed in \citet{agrawal1995sample} and the first finite-time analysis for the stochastic setting for this class of algorithms was proved in \citet{auer2002finite}. Over the years several strong UCB type algorithms were proposed for the stochastic setting such as MOSS \citep{audibert2009minimax}, UCBV \citep{audibert2009exploration}, UCB-Improved \citep{auer2010ucb}, OCUCB \citep{lattimore2015optimally}, etc. Further, some of the Bayesian strategies which were proposed for this setting includes the Thompson Sampling \citep{agrawal2012analysis},\citep{agrawal2013further} and Bayes-UCB \citep{kaufmann2012bayesian} algorithms.

Another setting that has greatly motivated the first studies in bandit literature is the \textit{adversarial setting} which was briefly described in chapter~\ref{chap:intro}. In this setting, at every timestep, an adversary chooses the reward for each arm and then the learner selects an arm without the knowledge of the adversary's choice. The adversary may or may not be oblivious to the learner's strategy and this forces the learner to employ a randomized algorithm to confuse the adversary. Previous works on this have focused on constructing different types of exponential weighting algorithms that are based on the Hedge algorithm that has been proposed before in \citet{littlestone1994weighted},\citet{freund1995desicion} and analyzed in \citet{auer1995gambling}. Further variants of this strategy called EXP3 \citep{auer2002nonstochastic} and EXP3IX \citep{kocak2014efficient} have also been proposed which incorporates different strategies for exploration to minimize the loss of the learner.

Striding between these two contrasting settings is the piece-wise stochastic multi-armed bandit setting where there are a finite number of changepoints when the distribution associated with each arm changes abruptly. Hence, this setting is neither as pessimistic as adversarial setting nor as optimistic as the stochastic setting. Therefore, the two broad class of algorithms mentioned before fail to perform optimally in this setting. Several interesting solutions have been proposed before for this setting which can be broadly divided into two categories, passively adaptive and actively adaptive strategies. Passively adaptive strategies like Discounted UCB (DUCB) \citep{kocsis2006discounted}, Switching Window UCB (SW-UCB)  \citep{garivier2011upper} and Discounted Thompson Sampling (DTS) \citep{raj2017taming} do not actively try to locate the changepoints but rather try to minimize their losses by concentrating on past few observations. Similarly, algorithms like Restarting Exp3 (RExp3) \citep{DBLP:journals/corr/BesbesGZ14} behave pessimistically as like Exp3 but restart after pre-determined phases. Hence, RExp3 can also be termed as a passively adaptive algorithm. On the other hand, actively adaptive strategies like Adapt-EVE \citep{hartland2007change}, Windowed-Mean Shift \citep{yu2009piecewise}, EXP3.R \citep{allesiardo2017non}, CUSUM-UCB \citep{liu2017change} try to locate the changepoints and restart the chosen bandit algorithms. Also, there are Bayesian strategies like Global Change-Point Thompson Sampling (GCTS)\citep{mellor2013thompson} which uses Bayesian changepoint detection to locate the changepoints. 