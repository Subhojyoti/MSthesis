Given the exploration-exploitation-changepoint dilemma, the objective of the learner in the piecewise-stochastic bandit problem is to minimize the cumulative regret until time $T$, which is defined as follows:
\begin{align*}
R_{T}=\sum_{t'=1}^T r_{i^*_{t'}} - \sum_{t'=1}^T r_{\mathbb{I}_t'}
%R_{t}=\sum_{t'=1}^t r_{i^*_{t'}} - \sum_{i=1}^K \left(r_{i}n_{i_{t'}\neq i^*_{t'},\forall t'=1:t}\right),
\end{align*}
where $T$ is the total number of rounds or horizon, $r_{i^*_{t'}}$ is the optimal arm at the $t'$ timestep and $r_{\mathbb{I}_{t'}}$ is the arm chosen by the learner at the $t'$ timestep. Let $n_{i_{t'}\neq i^*_{t'},\forall t'=1:T}$ is the number of times the learner has chosen arm $i$ up to round $T$ when it was not the optimal arm $i^*_{t'}$. The expected regret of an algorithm after $T$ rounds can be written as,

\begin{align*}
\E[R_{T}]&= \E\left[\sum_{t'=1}^T r_{i^*_{t'}} - \sum_{t'=1}^t r_{\mathbb{I}_t'}\right]\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
&=\sum_{i = 1}^K\left(r_{i^*} - r_i\right) \E[n_{i_{t'}\neq i^*_{t'},\forall t'=1:T}] \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
&\overset{(a)}{=}\sum_{i = 1}^K\sum_{j=1}^{G}\Delta^{opt}_{i,c_j}\E[n_{i_{t',c_j}\neq i^*_{t',c_j},\forall t'=t_{c_{j-1}}:t_{c_j}}]
\end{align*}
where $(a)$ is obtained from assumption \ref{assm:chg-gap} and $\Delta^{opt}_{i,c_j}$ is defined as in Definition \ref{Def:opt-gap}. 
