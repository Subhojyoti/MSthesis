Given the exploration-exploitation-changepoint dilemma, the objective of the learner in the piecewise-stochastic bandit problem is to minimize the cumulative regret until time $t$, which is defined as follows:
\begin{align*}
R_{t}=\sum_{t'=1}^t\mu_{i^*_{t'}} - \sum_{t'=1}^t\mu_{I_t'}
%R_{t}=\sum_{t'=1}^t\mu_{i^*_{t'}} - \sum_{i=1}^K \left(\mu_{i}n_{i_{t'}\neq i^*_{t'},\forall t'=1:t}\right),
\end{align*}
where $t$ is the number of rounds, $\mu_{i^*_{t'}}$ is the optimal arm at the $t'$ timestep and $\mu_{I_{t'}}$ is the arm chosen by the learner at the $t'$ timestep. Let $n_{i_{t'}\neq i^*_{t'},\forall t'=1:t}$ is the number of times the learner has chosen arm $i$ up to round $t$ when it was not the optimal arm $i^*_{t'}$. The expected regret of an algorithm after $t$ rounds can be written as,

\begin{align*}
\E[R_{t}]&= \E\left[\sum_{t'=1}^t\mu_{i^*_{t'}} - \sum_{t'=1}^t\mu_{I_t}\right]\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
&=\sum_{i\neq i^*}^K\left(\mu_{i^*} - \mu_i\right) \E[n_{i_{t'}\neq i^*_{t'},\forall t'=1:t}] \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
&\overset{(a)}{=}\sum_{i\neq i^*}^K\sum_{j=1}^{G}\Delta^{opt}_{i,c_j}\E[n_{i_{t'}\neq i^*_{t'},\forall t'=t_{c_{j-1}}:t_{c_j}}]
\end{align*}
where $(a)$ is obtained from assumption \ref{assm:chg-gap} and $\Delta^{opt}_{i,c_j}$ is defined as in Definition \ref{Def:opt-gap}. 
