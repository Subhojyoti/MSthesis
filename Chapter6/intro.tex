In this chapter, we consider the piece-wise stochastic multi-armed bandit problem, an interesting variation of the stochastic multi-armed bandit (SMAB) problem in sequential decision making which was discussed in detail in chapter \ref{chap:SMAB}. In this setting,  a learning algorithm is provided with a set of decisions (or arms) with reward distributions unknown to the learner. The learning proceeds in an iterative fashion, where in each round, the algorithm chooses an arm and receives a stochastic reward that is drawn from a distribution specific to the arm selected. There exist a finite number of changepoints such that the reward distribution of arms changes at those changepoints. Given the goal of maximizing the cumulative reward, the learner faces the \textit{exploration-exploitation-changepoint} dilemma, as opposed to the simple \textit{exploration-exploitation} dilemma, i.e., in each round should the algorithm select the arm which has the highest observed reward so far (\textit{exploitation}), or should the algorithm choose a new arm to gain more knowledge of the expected reward of the arms and thereby avert a sub-optimal greedy decision (\textit{exploration}) and finally keep track of the \textit{changepoints} and adapt accordingly.

    The rest of the chapter is organized as follows. We first state the notations, definitions, and assumptions required for this setting in section~\ref{psbandit:notations}. Then we define our problem statement in section~\ref{psbandit:probDef} and in section~\ref{psbandit:related} we discuss the related works in this setting. We elaborate our contributions in section~\ref{psbandit:contribution} and in section~\ref{psbandit:algorithm} we present the changepoint detection algorithms. Section~\ref{psbandit:results} contains our main result, Section~\ref{psbandit:expt} contains numerical simulations where we test our proposed algorithms and finally we summarize in section \ref{psbandit:conclusion}. 

