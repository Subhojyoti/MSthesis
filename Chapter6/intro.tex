In this paper, we consider the piece-wise stochastic multi-armed bandit problem, an interesting variation of the stochastic multi-armed bandit problem in sequential decision making. In this setting,  a learning algorithm is provided with a set of decisions (or arms) with reward distributions unknown to the learner. The learning proceeds in an iterative fashion, where in each round, the algorithm chooses an arm and receives a stochastic reward that is drawn from a distribution specific to the arm selected. There exist a finite number of changepoints such that the reward distribution of arms changes at those changepoints. Given the goal of maximizing the cumulative reward, the learner faces the \textit{exploration-exploitation-changepoint} dilemma, i.e., in each round should the algorithm select the arm which has the highest observed reward so far (\textit{exploitation}), or should the algorithm choose a new arm to gain more knowledge of the expected reward of the arms and thereby avert a sub-optimal greedy decision (\textit{exploration}) and finally keep track of the \textit{changepoints}. 

