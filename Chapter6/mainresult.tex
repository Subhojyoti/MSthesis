%\begin{lemma}
%
%\end{lemma}

\begin{theorem}
\label{psbandit:Theorem:1}
The expected cumulative regret of ImpCPD algorithm is upper bounded by,
\begin{align*}
\E[R_t] &\leq \sum_{i = 1}^K\sum_{j=1}^{G}\bigg[ 1 
%%%%%%%%%%
+ \dfrac{4T^{\frac{3}{2}}\Delta^{opt}_{i,c_j}\log(\psi T (\Delta^{opt}_{i,c_j})^4)}{(\psi)^{\alpha}}
%%%%%%%%%%
+ \Delta^{opt}_{i,c_j} + \dfrac{4\log(\psi T(\Delta^{opt}_{i,c_j})^4)}{2(\Delta^{opt}_{i,c_j})}\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%
& + \dfrac{2T^{\frac{3}{2}}\Delta^{opt}_{i,c_{j+1}}\log(\psi T (\Delta^{chg}_{i,c_j})^4)}{(\psi)^{\alpha}}\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%
&+ \Delta^{opt}_{\max,c_{j+1}} + \dfrac{\Delta^{opt}_{\max,c_{j+1}}\log(\psi T(\Delta^{chg}_{i,c_j})^4)}{2(\Delta^{chg}_{\epsilon_0,c_j})^2} + \Delta^{opt}_{i,c_{j+1}} + \dfrac{\Delta^{opt}_{i,c_{j+1}}\log(\psi T(\Delta^{chg}_{i,c_j})^4)}{2(\Delta^{chg}_{i,c_j})^2}
\bigg]
\end{align*}

where $\alpha,\psi$ are exploration parameters and $\Delta^{opt}_{i,c_{G+1}} = 0,\forall i\in\A$.
\end{theorem}

\begin{proof}

We proceed as like Theorem 3.1 in \citet{auer2010ucb} and we combine the changepoint detection sub-routine with this. 

\textbf{Step 0.(Vital Assumption for proving):} In this proof, we assume that all the changepoints are detected by the algorithm. 

\textbf{Step 1.(Define some notations):} In this proof, we define the confidence interval for the $i$-th arm as $s_i=\sqrt{\dfrac{\alpha\log(\psi T\epsilon_m^2)}{n_{i}}}$. The phase numbers are denoted by $m=0,1,\ldots,M$ where $M=\frac{1}{2}\log_{2}\frac{T}{e}$. For the sake of brevity, in this proof we write $n_i$ instead of $n_{i,t_0:t_p}$ denoting only the number of times the arm $i$ is pulled between two restarts, ie, between $t_0$ to $t_p$.

\textbf{Step 2.(Define an optimality stopping phase $m_{i,c_j}$):} We define an optimality stopping phase $m_{i,c_j}$ for a sub-optimal arm $i\in\A$ as the first phase after which the arm $i$ is no longer pulled till the $c_j$-th changepoint is detected such that,

\begin{align*}
m_{i,c_j} = \min\left\lbrace m: \sqrt{\alpha\epsilon_{m}} < \dfrac{\Delta^{opt}_{i,c_j}}{4} \right\rbrace
\end{align*} 

\textbf{Step 3.(Define a changepoint stopping phase $p_{i,c_j}$):} We define a changepoint stopping phase $p_{i,c_j}$ for an arm $i\in\A$ such that it is the first phase when the changepoint $c_j$ is detected such that,

\begin{align*}
p_{i,c_j} = \min\left\lbrace m: \sqrt{\alpha\epsilon_{m}} < \dfrac{\Delta^{chg}_{i,c_j}}{4} \right\rbrace
\end{align*}

\textbf{Step 4.(Regret for sub-optimal arm $i$ being pulled after $m_{i,c_j}$-th phase):} Note, that on or after the $m_{i,c_j}$-th phase a sub-optimal arm $i$ is not pulled anymore if these four conditions hold,

\begin{eqnarray}
\hat{r}_{i,c_j} < r_{i,c_j} + s_i, \hspace*{2em}  \hat{r}_{i*,c_j} > r_{i*,c_j} - s_{i*}, \hspace*{2em} s_i > s_{i*}, \hspace*{2em} n_i < n_{i*} \label{eq:arm-pull-opt}
\end{eqnarray}

Also, in the $m_{i,c_j}$-th phase if $n_i \geq \ell_{m_{i,c_j}} = \dfrac{\log(\psi T\epsilon_{m_{i,c_j}}^2)}{2\epsilon_{m_{i,c_j}}}$ then we can show that,
\begin{align*}
s_i = \sqrt{\dfrac{\alpha\log(\psi T\epsilon_{m_{i,c_j}}^2)}{2n_{i}}} \leq \sqrt{\dfrac{\alpha\log(\psi T\epsilon_{m_{i,c_j}}^2)}{2\ell_{m_{i,c_j}}}} \leq \sqrt{\alpha\epsilon_{m_{i,c_j}}\dfrac{\log(\psi T\epsilon_{m_{i,c_j}}^2)}{\log(\psi T\epsilon_{m_{i,c_j}}^2)}} \leq \dfrac{\Delta^{opt}_{i,c_j}}{4}.
\end{align*}

If indeed the four conditions in equation \ref{eq:arm-pull-opt} hold then we can show that in the $m_{i,c_j}$-th phase,

\begin{align*}
\hat{r}_{i,c_j} + s_i \leq {r}_{i,c_j} + 4s_i - 2s_i \leq {r}_{i,c_j} + \Delta^{opt}_{i,c_j} - 2s_i \leq {r}_{i*,c_j} - 2s_{i*} \leq \hat{r}_{i*,c_j} - s_{i*}
\end{align*}

Hence, the sub-optimal arm $i$ is no longer pulled on or after the $m_{i,c_j}$-th phase. Therefore, to bound the number of pulls of the sub-optimal arm $i$, we need to bound the probability of the complementary of the four events in equation \ref{eq:arm-pull-chg}.

For the first event in equation \ref{eq:arm-pull-opt}, using Chernoff-Hoeffding bound we can bound the probability of the complementary of that event by,

\begin{align*}
&\sum_{m=0}^{m_{i,c_j}}\sum_{n_i=1}^{\ell_{m_{i,c_j}}}\Pb\lbrace \hat{r}_{i,c_j} \geq  r_{i,c_j} + s_i \rbrace \leq \sum_{m=0}^{m_{i,c_j}}\sum_{n_i=1}^{\ell_{m_{i,c_j}}}\exp\left(-2s_i^2n_i \right)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\leq \sum_{m=0}^{m_{i,c_j}}\sum_{n_i=1}^{\ell_{m_{i,c_j}}}\exp\left(-2\dfrac{\alpha\log(\psi T\epsilon_{m_{i,c_j}}^2)}{2n_i}n_i \right)\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
&\leq \sum_{m=0}^{m_{i,c_j}}\sum_{n_i=1}^{\ell_{m_{i,c_j}}}\dfrac{1}{\psi T\epsilon_{m_{i,c_j}}^2} \leq \sum_{m=0}^{m_{i,c_j}}\dfrac{\log(\psi T\epsilon_{m_{i,c_j}}^2)}{2\epsilon_{m_{i,c_j}}}\dfrac{1}{(\psi T\epsilon_{m_{i,c_j}}^2)^{\alpha}} = \dfrac{\log(\psi T \epsilon_{m_{i,c_j}}^2)}{2(\psi T)^{\alpha}}\sum_{m=0}^{m_{i,c_j}}\dfrac{1}{\epsilon_{m_{i,c_j}}^{2\alpha +1}}.
\end{align*}

Similarly, for the second event in equation \ref{eq:arm-pull-opt}, we can bound the probability of its complementary event by,

\begin{align*}
\sum_{m=0}^{m_{i,c_j}}\sum_{n_{i*} =1}^{\ell_{m_{i,c_j}}}\Pb\lbrace \hat{r}_{i*,c_j} \leq  r_{i*,c_j} - s_{i*} \rbrace &\leq \sum_{m=0}^{m_{i,c_j}}\sum_{n_i=1}^{\ell_{m_{i,c_j}}}\exp\left(-2(s_{i*})^2n_{i*} \right)\leq \dfrac{\log(\psi T \epsilon_{m_{i,c_j}}^2)}{2(\psi T)^{\alpha}}\sum_{m=0}^{m_i}\dfrac{1}{\epsilon_{m_i}^{2\alpha +1}}.
\end{align*}


Also, for the third event in equation \ref{eq:arm-pull-opt}, we can bound the probability of its complementary event by,

\begin{align*}
\sum_{m=0}^{m_{i,c_j}}\sum_{n_i=1}^{\ell_{m_{i,c_j}}}\Pb\lbrace s_i < s_{i*}\rbrace \leq \sum_{m=0}^{m_{i,c_j}}\sum_{n_i=1}^{\ell_{m_{i,c_j}}}\Pb\lbrace n_i > n_{i*}\rbrace \leq \sum_{m=0}^{m_{i,c_j}}\sum_{n_i=1}^{\ell_{m_{i,c_j}}}\Pb\lbrace \hat{r}_{i,c_j} + s_i > \hat{r}_{i*,c_j} + s_{i*} \rbrace.
\end{align*}

But the event $\hat{r}_{i,c_j} + s_i > \hat{r}_{i*,c_j} + s_i$ is possible only when the following three events occur, $\hat{r}_{i*,c_j} \leq r_{i*,c_j} - s_{i*} , \hat{r}_{i,c_j} \geq r_{i,c_j} + s_i$ and $r_{i*}-r_{i} < 2s_i $. But the third event will not happen for $n_i\geq \ell_{m_{i,c_j}}$. Proceeding as before, we can show that the probability of the remaining two events is bounded by,

\begin{align*}
&\sum_{m=0}^{m_{i,c_j}}\sum_{n_i=1}^{\ell_{m_{i,c_j}}}\Pb\lbrace s_i < s_{i*}\rbrace \leq \sum_{m=0}^{m_{i,c_j}}\sum_{n_i=1}^{\ell_{m_{i,c_j}}}\Pb\lbrace n_i > n_{i*}\rbrace \leq \sum_{m=0}^{m_{i,c_j}}\sum_{n_i=1}^{\ell_{m_{i,c_j}}}\Pb\lbrace \hat{r}_{i,c_j} + s_i > \hat{r}_{i*,c_j} + s_{i*} \rbrace \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
&\leq \sum_{m=0}^{m_{i,c_j}}\sum_{n_i=1}^{\ell_{m_{i,c_j}}}\left( \Pb\lbrace \hat{r}_{i,c_j} \geq r_{i,c_j} + s_i \rbrace \bigcup \Pb\lbrace \hat{r}_{i*,c_j} \leq r_{i*,c_j} - s_{i*} \rbrace \right)\leq 2\left(\dfrac{\log(\psi T \epsilon_{m_{i,c_j}}^2)}{2(\psi T)^{\alpha}}\right)\sum_{m=0}^{m_{i,c_j}}\dfrac{1}{\epsilon_{m_{i,c_j}}^{2\alpha + 1}}.
\end{align*}

%\sum_{m=0}^{m_{i,c_j}}\sum_{n_i=1}^{\ell_{m_{i,c_j}}}
Finally for the fourth event in equation \ref{eq:arm-pull-opt}, we can bound the probability of its complementary event by following the same steps as above,

\begin{align*}
\sum_{m=0}^{m_i}\sum_{n_i=1}^{\ell_{m_{i,c_j}}}\Pb\lbrace n_i > n_{i*}\rbrace \leq \sum_{m=0}^{m_{i,c_j}}\sum_{n_i=1}^{\ell_{m_{i,c_j}}}\Pb\lbrace r_{i,c_j} + s_i > r_{i*,c_j} + s_{i*} \rbrace \leq \dfrac{\log(\psi T \epsilon_{m_{i,c_j}}^2)}{(\psi T)^{\alpha}}\sum_{m=0}^{m_{i,c_j}}\dfrac{1}{\epsilon_{m_i}^{2\alpha +1}}.
\end{align*}


Combining the above four cases we can bound the probability that a sub-optimal arm $i$ will no longer be pulled on or after the $m_{i,c_j}$-th phase by,

\begin{align*}
\dfrac{4\log(\psi T \epsilon_{m_{i,c_j}}^2)}{(\psi T)^{\alpha}}\sum_{m=0}^{m_{i,c_j}}\dfrac{1}{\epsilon_{m_{i,c_j}}^{2\alpha +1}} &\leq \dfrac{4\log(\psi T \epsilon_{m_{i,c_j}}^2)}{(\psi T)^{\alpha}}\sum_{m=0}^{M}\left(\dfrac{1}{\epsilon_{m}}\right)^{2\alpha +1} \overset{(a)}{\leq} \dfrac{4\log(\psi T \epsilon_{m_{i,c_j}}^2)}{(\psi T)^{\alpha}}\left(\dfrac{2(2^M - 1)}{2 - 1}\right)^{2\alpha +1} \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%
&\overset{(b)}{\leq} \dfrac{4\log(\psi T \epsilon_{m_{i,c_j}}^2)}{(\psi T)^{\alpha}}\left(\sqrt{T}\right)^{2\alpha +1} = \dfrac{4\sqrt{T}\log(\psi T \epsilon_{m_{i,c_j}}^2)}{(\psi)^{\alpha}}.
\end{align*}

Here, in $(a)$ we use the standard geometric progression formula and in $(b)$ we substitute the value of $M=\dfrac{1}{2}\log_{2}\dfrac{T}{e}$. Bounding this trivially by $T\Delta^{opt}_{i,c_j}$ for each arm $i\in\A$ we get the regret suffered for arm $i$ after the $m_{i,c_j}$-th phase  as,

\begin{align*}
\sum_{i\in\A}\left( \dfrac{4T\Delta^{opt}_{i,c_j}\sqrt{T}\log(\psi T \epsilon_{m_{i,c_j}}^2)}{(\psi)^{\alpha}} \right) \leq \sum_{i\in\A}\left( \dfrac{4T^{\frac{3}{2}}\Delta^{opt}_{i,c_j}\log(\psi T (\Delta^{opt}_{i,c_j})^4)}{(\psi)^{\alpha}} \right).
\end{align*}


%\begin{align*}
%\sum_{i\in\A}\left(\dfrac{4T\Delta^{opt}_{i,c_j}\log(\psi T)}{(\psi T)^{\alpha}}\sum_{m=0}^{m_{i,c_j}}\dfrac{1}{\epsilon_{m_{i,c_j}}^{2\alpha +1}}\right) \leq \sum_{i\in\A}\left(\dfrac{4T\Delta^{opt}_{i,c_j}\log(\psi T)}{(\psi T)^{\alpha}}\dfrac{M 2^{2\alpha +1}}{(\Delta^{opt}_{i,c_j})^{4\alpha +2}}\right).
%\end{align*}

\textbf{Step 5.(Regret for pulling the sub-optimal arm $i$ on or before $m_{i,c_j}$-th phase):} Either a sub-optimal arm gets pulled $\ell_{m_{i,c_j}}$ number of times till the $m_{i,c_j}$-th phase or after that the probability of it getting pulled is exponentially low (as shown in \textbf{step 4}). Hence, the number of times a sub-optimal arm $i$ is pulled till the $m_{i,c_j}$-th phase is given by,

\begin{align*}
n_i < \ell_{m_{i,c_j}} = \left\lceil \dfrac{\log(\psi T\epsilon_{m_{i,c_j}}^2)}{2\epsilon_{m_{i,c_j}}} \right\rceil
\end{align*}

Hence, considering each arm $i\in\A$ the total regret is bounded by,

\begin{align*}
\sum_{i\in\A}\Delta^{opt}_{i,c_j}\left\lceil \dfrac{\log(\psi T\epsilon_{m_{i,c_j}}^2)}{2\epsilon_{m_{i,c_j}}} \right\rceil < \Delta^{opt}_{i,c_j}\left[ 1 + \dfrac{\log(\psi T\epsilon_{m_{i,c_j}}^2)}{2\epsilon_{m_{i,c_j}}} \right] \leq \Delta^{opt}_{i,c_j}\left[ 1 + \dfrac{4\log(\psi T(\Delta^{opt}_{i,c_j})^4)}{2(\Delta^{opt}_{i,c_j})^2}\right].
\end{align*} 

\textbf{Step 6.(Regret for not detecting changepoint $c_{j}$ for arm $i$ after the $p_{i,c_j}$-th phase):} Proceeding similarly as in \textbf{step 4} we can show that in the $p_{i,c_j}$-th phase for an arm $i$ if $n_{i}\geq\ell_{p_{i,c_j}}$ then,

\begin{align*}
s_i = \sqrt{\dfrac{\alpha\log(\psi T\epsilon_{p_{i,c_j}}^2)}{2n_{i}}} \leq \sqrt{\dfrac{\alpha\log(\psi T\epsilon_{p_{i,c_j}}^2)}{2\ell_{p_{i,c_j}}}} \leq \sqrt{\alpha\epsilon_{p_{i,c_j}}\dfrac{\log(\psi T\epsilon_{p_{i,c_j}}^2)}{\log(\psi T\epsilon_{p_{i,c_j}}^2)}} \leq \dfrac{\Delta^{chg}_{i,c_j}}{4}.
\end{align*}

Furthermore, we can show that if the following two conditions hold for an arm $i$ then the changepoint will definitely get detected, that is,

\begin{eqnarray}
\hat{r}_{i,c_{j-1}} + s_{i} < \hat{r}_{i,c_{j}} - s_{i},\hspace*{4mm} \hat{r}_{i,c_{j-1}} - s_{i} > \hat{r}_{i,c_{j}} + s_{i} \label{eq:arm-pull-chg}.
\end{eqnarray}

Indeed, we can show that if there is a changepoint at $c_j$ such that $r_{i,c_{j-1}} < r_{i,c_j}$ and if the first conditions in equation \ref{eq:arm-pull-chg} hold  in the $p_{i,c_j}$-th phase then,

\begin{align*}
\hat{r}_{i,c_{j-1}} + s_{i} \leq {r}_{i,c_{j-1}} + 4s_{i} - 2s_{i} \leq {r}_{i,c_{j-1}} + \Delta^{chg}_{i,c_j} -2s_{i} \leq {r}_{i,c_{j}} - 2s_i \leq \hat{r}_{i,c_{j}} - s_i.
\end{align*}

Conversely, for the case $r_{i,c_{j-1}} > r_{i,c_j}$ the second condition will hold for the $p_{i,c_j}$-th phase. Now, to bound the regret we need to bound the probability of the complementary of these two events. For the complementary of the first event in equation \ref{eq:arm-pull-chg} by using Chernoff-Hoeffding bound we can show that,

\begin{align*}
&\sum_{m=0}^{p_{i,c_j}}\sum_{n_i=1}^{\ell_{p_{i,c_j}}}\Pb\lbrace \hat{r}_{i,c_{j-1}} \geq  r_{i,c_{j-1}} + s_i \rbrace \leq \sum_{m=0}^{p_{i,c_j}}\sum_{n_i=1}^{\ell_{p_{i,c_j}}}\exp\left(-2s_i^2n_i \right)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\leq \sum_{m=0}^{p_{i,c_j}}\sum_{n_i=1}^{\ell_{p_{i,c_j}}}\exp\left(-2\dfrac{\alpha\log(\psi T\epsilon_{p_{i,c_j}}^2)}{2n_i}n_i \right)\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
&\leq \sum_{m=0}^{p_{i,c_j}}\sum_{n_i=1}^{\ell_{p_{i,c_j}}}\dfrac{1}{\psi T\epsilon_{p_{i,c_j}}^2} \leq \sum_{m=0}^{p_{i,c_j}}\dfrac{\log(\psi T\epsilon_{p_{i,c_j}}^2)}{2\epsilon_{p_{i,c_j}}}\dfrac{1}{(\psi T)^{\alpha}\epsilon_{p_{i,c_j}}^2} = \dfrac{\log(\psi T\epsilon_{p_{i,c_j}}^2)}{2(\psi T)^{\alpha}}\sum_{m=0}^{p_{i,c_j}}\dfrac{1}{\epsilon_{p_{i,c_j}}^{2\alpha + 1}}.
\end{align*}

Also, again by using Chernoff-Hoeffding bound we can show that,

\begin{align*}
\sum_{m=0}^{p_{i,c_j}}\sum_{n_i=1}^{\ell_{p_{i,c_j}}}\Pb\lbrace \hat{r}_{i,c_j} \leq  r_{i,c_j} - s_i \rbrace &\leq \sum_{m=0}^{p_{i,c_j}}\sum_{n_i=1}^{\ell_{p_{i,c_j}}}\exp\left(-2s_i^2n_i \right) \leq \dfrac{\log(\psi T\epsilon_{p_{i,c_j}}^2)}{2(\psi T)^{\alpha}}\sum_{m=0}^{p_{i,c_j}}\dfrac{1}{\epsilon_{p_{i,c_j}}^{2\alpha + 1}}.
\end{align*}

%\ell_{p_{i-1, c_j}}
Hence, the probability that the changepoint is not detected by the first event in equation \ref{eq:arm-pull-chg} is bounded by,
\begin{align*}
\dfrac{\log(\psi T)}{(\psi T \epsilon_{p_{i,c_j}}^2)^{\alpha}}\sum_{m=0}^{p_{i,c_j}}\dfrac{1}{\epsilon_{p_{i,c_j}}^{2\alpha + 1}}.
\end{align*}

Similarly, we can bound the probability of the complementary of the second event in equation \ref{eq:arm-pull-chg} as,

\begin{align*}
&\sum_{m=0}^{p_{i,c_j}}\sum_{n_i=1}^{\ell_{p_{i,c_j}}}\Pb\lbrace \hat{r}_{i,c_{j-1}} \leq  r_{i,c_{j-1}} - s_i \rbrace\leq \dfrac{\log(\psi T\epsilon_{p_{i,c_j}}^2)}{2(\psi T)^{\alpha}}\sum_{m=0}^{p_i}\dfrac{1}{\epsilon_{p_{i,c_j}}^{2\alpha + 1}} \textbf{,  and  }\\
%%%%%%%%%%%%%%%%%%%%%%%%%
&\sum_{m=0}^{p_{i,c_j}}\sum_{n_i=1}^{\ell_{p_{i,c_j}}}\Pb\lbrace \hat{r}_{i,c_j} \geq  r_{i,c_j} + s_i \rbrace\leq \dfrac{\log(\psi T\epsilon_{p_{i,c_j}}^2)}{(\psi T)^{\alpha}}\sum_{m=0}^{p_{i,c_j}}\dfrac{1}{\epsilon_{p_{i,c_j}}^{2\alpha + 1}}.
\end{align*}

%\ell_{p_{i-1,c_j}}

Combining the contribution from these two events, we can show that the probability of not detecting the changepoint $c_j$ for the $i$-th arm after the $p_{i,c_j}$-th phase is upper bounded by,

\begin{align*}
\dfrac{2\log(\psi T)}{(\psi T\epsilon_{p_{i,c_j}}^2)^{\alpha}}\sum_{m=0}^{p_{i,c_j}}\dfrac{1}{\epsilon_{p_{i,c_j}}^{2\alpha + 1}} \overset{(a)}{\leq} \dfrac{2\sqrt{T}\log(\psi T \epsilon_{p_{i,c_j}}^2)}{(\psi)^{\alpha}} .
\end{align*}

Here, in $(a)$ we follow the same steps as in \textbf{step 4} to reduce the expression to the above form. Furthermore, bounding the regret trivially (after the changepoint $c_j$) by $\Delta^{opt}_{i,c_{j+1}}$ for each arm $i\in\A$, we get 


\begin{align*}
\sum_{i\in\A}\left( \dfrac{2T\Delta^{opt}_{i,c_{j+1}}\sqrt{T}\log(\psi T \epsilon_{p_{i,c_j}}^2)}{(\psi)^{\alpha}} \right) \leq \sum_{i\in\A}\left( \dfrac{2T^{\frac{3}{2}}\Delta^{opt}_{i,c_{j+1}}\log(\psi T (\Delta^{chg}_{i,c_j})^4)}{(\psi)^{\alpha}} \right).
\end{align*}


%\begin{align*}
%\sum_{i\in\A}\left(\dfrac{2T\Delta^{opt}_{i,c_{j+1}}\log(\psi T)}{(\psi T)^{\alpha}}\sum_{m=0}^{p_{i,c_j}}\dfrac{1}{\epsilon_{p_{i,c_j}}^{2\alpha + 1}}\right) \leq \sum_{i\in\A}\left(\dfrac{2T\Delta^{opt}_{i,c_{j+1}}\log(\psi T)}{(\psi T)^{\alpha}}\dfrac{M 2^{2\alpha + 1}}{(\Delta^{chg}_{i,c_j})^{4\alpha + 2}}\right).
%\end{align*}

\textbf{Step 7.(Regret for not detecting a changepoint $c_{j}$ for arm $i$ on or before the $p_{i,c_j}$-th phase):} The regret for not detecting the changepoint $c_j$ on or before the $p_{i,c_j}$-th phase can be broken into two parts, \textbf{(a)} the worst case events from $t_{c_j}$ to $p_{i}$ and \textbf{(b)} the minimum number of pulls $\ell_{p_{i,c_j}}$ required to detect the changepoint. For the first part (a) we can use Assumption \ref{assm:space-gap}, Assumption ref{assm:chg-gap}, Discussion \ref{dis:gap-delay} and Definition \ref{Def:e-chg-gap} to upper bound the regret as,

\begin{align*}
\sum_{i\in\A}\Delta^{opt}_{\max,c_{j+1}}\left\lceil \dfrac{\log(\psi T\epsilon_{p_{i,c_j}}^2)}{2(\Delta^{chg}_{\epsilon_0,c_j})^2} \right\rceil < \Delta^{opt}_{\max,c_{j+1}}\left[ 1 + \dfrac{\log(\psi T\epsilon_{p_{i,c_j}}^2)}{2(\Delta^{chg}_{\epsilon_0,c_j})^2} \right].
\end{align*}

Again, for the second part (b) the arm $i$ can be pulled no more than $\ell_{p_i}$ number of times. Hence, for each arm $i\in\A$  the regret for this case is bounded by,

\begin{align*}
\sum_{i\in\A}\Delta^{opt}_{i,c_{j+1}}\left\lceil \dfrac{\log(\psi T\epsilon_{p_{i,c_j}}^2)}{2\epsilon_{p_{i,c_j}}} \right\rceil <  \Delta^{opt}_{i,c_{j+1}}\left[ 1 + \dfrac{\log(\psi T\epsilon_{p_i}^2)}{2\epsilon_{p_i}} \right].
\end{align*}

Therefore, combining these two parts (a) and (b) we can show that the total regret for not detecting the changepoint till the $p_{i,c_j}$-th phase is given by,

\begin{align*}
&\sum_{i\in\A}\left\lbrace\Delta^{opt}_{\max,c_{j+1}} + \dfrac{\Delta^{opt}_{\max,c_{j+1}}\log(\psi T\epsilon_{p_{i,c_j}}^2)}{2(\Delta^{chg}_{\epsilon_0,c_j})^2} + \Delta^{opt}_{i,c_{j+1}} + \dfrac{\Delta^{opt}_{i,c_{j+1}}\log(\psi T\epsilon_{p_{i,c_j}}^2)}{2\epsilon_{p_i}}\right\rbrace \\
%%%%%%%%%%%%%%%%%%%%%%%%%
&\leq \sum_{i\in\A}\left\lbrace\Delta^{opt}_{\max,c_{j+1}} + \dfrac{\Delta^{opt}_{\max,c_{j+1}}\log(\psi T(\Delta^{chg}_{i,c_j})^4)}{2(\Delta^{chg}_{\epsilon_0,c_j})^2} + \Delta^{opt}_{i,c_{j+1}} + \dfrac{\Delta^{opt}_{i,c_{j+1}}\log(\psi T(\Delta^{chg}_{i,c_j})^4)}{2(\Delta^{chg}_{i,c_j})^2}\right\rbrace
\end{align*}

\textbf{Step 8.(Final Regret bound):} Combining the assumption in \textbf{Step 0} and the problem definition, the expected regret till the $T$-th timestep is bounded by,

\begin{align*}
\E[R_{T}]&= \sum_{i = 1}^K\sum_{j=1}^{G}\Delta^{opt}_{i,c_j}\E[n_{i_{t',c_j}\neq i^*_{t',c_j},\forall t'=t_{c_{j-1}}:t_{c_j}}] \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%
& \leq \sum_{i = 1}^K\sum_{j=1}^{G}\bigg[ 1 
%%%%%%%%%%
+ \underbrace{ \dfrac{4T^{\frac{3}{2}}\Delta^{opt}_{i,c_j}\log(\psi T (\Delta^{opt}_{i,c_j})^4)}{(\psi)^{\alpha}} }_{\textbf{from Step 4}}
%%%%%%%%%%
+ \underbrace{\Delta^{opt}_{i,c_j} + \dfrac{4\log(\psi T(\Delta^{opt}_{i,c_j})^4)}{2(\Delta^{opt}_{i,c_j})}}_{\textbf{from Step 5}}\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%
& + \underbrace{ \dfrac{2T^{\frac{3}{2}}\Delta^{opt}_{i,c_{j+1}}\log(\psi T (\Delta^{chg}_{i,c_j})^4)}{(\psi)^{\alpha}} }_{\textbf{from Step 6}}\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%
&+ \underbrace{ \Delta^{opt}_{\max,c_{j+1}} + \dfrac{\Delta^{opt}_{\max,c_{j+1}}\log(\psi T(\Delta^{chg}_{i,c_j})^4)}{2(\Delta^{chg}_{\epsilon_0,c_j})^2} + \Delta^{opt}_{i,c_{j+1}} + \dfrac{\Delta^{opt}_{i,c_{j+1}}\log(\psi T(\Delta^{chg}_{i,c_j})^4)}{2(\Delta^{chg}_{i,c_j})^2}}_{\textbf{from Step 7}}
\bigg]
\end{align*}

Hence, we get the main result of the regret bound.

\end{proof}

\begin{corollary}
\label{psbandit:corollary:1}
In the worst case scenario, when all the gaps are same, that is $\Delta^{opt}_{i,c_j}=\Delta^{chg}_{i,c_j}=\sqrt{\dfrac{K\log K}{T}}, \forall i\in\A,\forall j\in\G$ and setting $\alpha=\dfrac{1}{2}$ and $\psi = \dfrac{T^2}{K^2}$ then the worst case gap-independent regret bound of ImpCPD is given by,

\begin{align*}
\E[R_T]\leq 3G\sqrt{\dfrac{K^3\log K}{T}} + 24G\sqrt{KT\log K}\left(\log\left(\sqrt{T}\right)\right) + 16GK^2 \sqrt{(K\log K)}\left(\log\left( \sqrt{T}(\log K)\right) \right).
\end{align*}

\end{corollary}

\begin{proof}

We first recall the result of Theorem \ref{psbandit:Theorem:1} below,

\begin{align*}
\E[R_{T}] &\leq \sum_{i = 1}^K\sum_{j=1}^{G}\bigg[ 1 
%%%%%%%%%%
+ \underbrace{ \dfrac{4T^{\frac{3}{2}}\Delta^{opt}_{i,c_j}\log(\psi T (\Delta^{opt}_{i,c_j})^4)}{(\psi)^{\alpha}} }_{\textbf{part A}}
%%%%%%%%%%
+ \underbrace{\Delta^{opt}_{i,c_j} + \dfrac{4\log(\psi T(\Delta^{opt}_{i,c_j})^4)}{2(\Delta^{opt}_{i,c_j})}}_{\textbf{part B}}\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%
& + \underbrace{ \dfrac{2T^{\frac{3}{2}}\Delta^{opt}_{i,c_{j+1}}\log(\psi T (\Delta^{chg}_{i,c_j})^4)}{(\psi)^{\alpha}} }_{\textbf{part C}}\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%
&+ \underbrace{ \Delta^{opt}_{\max,c_{j+1}} + \dfrac{\Delta^{opt}_{\max,c_{j+1}}\log(\psi T(\Delta^{chg}_{i,c_j})^4)}{2(\Delta^{chg}_{\epsilon_0,c_j})^2} + \Delta^{opt}_{i,c_{j+1}} + \dfrac{\Delta^{opt}_{i,c_{j+1}}\log(\psi T(\Delta^{chg}_{i,c_j})^4)}{2(\Delta^{chg}_{i,c_j})^2}}_{\textbf{part D}}
\bigg]
\end{align*}

Then, substituting $\Delta^{opt}_{i,c_j}=\Delta^{chg}_{i,c_j}=\sqrt{\dfrac{K\log K}{T}}, \forall i\in\A,\forall j\in\G$ and $\alpha=\dfrac{1}{2}$ and $\psi = \dfrac{T^2}{K^2}$ we can show that for \textbf{part A},

\begin{align*}
&G\sum_{i\in\A}\underbrace{ \dfrac{4T^{\frac{3}{2}}\Delta^{opt}_{i,c_j}\log(\psi T (\Delta^{opt}_{i,c_j})^4)}{(\psi)^{\alpha}} }_{\textbf{part A}} \leq \dfrac{4GKT^{\frac{3}{2}}\sqrt{\dfrac{K\log K}{T}}\log(\dfrac{T^2}{K^2} T (\sqrt{\dfrac{K\log K}{T}})^4)}{(\dfrac{T^2}{K^2})^{\frac{1}{2}}}\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
  &= 4GK^2 \sqrt{(K\log K)}(\log\left( T(\log K)^2\right) ) = 8K^2 \sqrt{(K\log K)}\left(\log\left( \sqrt{T}(\log K)\right) \right).
\end{align*}

Again, for \textbf{part B} we can show that,

\begin{align*}
&G\sum_{i\in\A}\left(\underbrace{\Delta^{opt}_{i,c_j} + \dfrac{4\log(\psi T(\Delta^{opt}_{i,c_j})^4)}{2(\Delta^{opt}_{i,c_j})}}_{\textbf{part B}}\right) \leq GK\sqrt{\dfrac{K\log K}{T}} + \dfrac{4GK\log(\dfrac{T^2}{K^2} T(\sqrt{\dfrac{K\log K}{T}})^4)}{2(\sqrt{\dfrac{K\log K}{T}})} \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
&= GK\sqrt{\dfrac{K\log K}{T}} + \dfrac{8G\sqrt{KT}(\log\left( \sqrt{T}(\log K)\right)) }{\sqrt{\log K}} = G\sqrt{\dfrac{K^3\log K}{T}} + 8G\sqrt{KT\log K}\left(\log\left(\sqrt{T}\right)\right).
\end{align*}

Similarly to \textbf{part A}, we can reduce \textbf{part C} as,

\begin{align*}
G\sum_{i\in\A}\underbrace{ \dfrac{2T^{\frac{3}{2}}\Delta^{opt}_{i,c_{j+1}}\log(\psi T (\Delta^{chg}_{i,c_j})^4)}{(\psi)^{\alpha}} }_{\textbf{part C}} \leq 8GK^2 \sqrt{(K\log K)}\left(\log\left( \sqrt{T}(\log K)\right) \right).
\end{align*}

And finally, for \textbf{part D}, similar to \textbf{part B}, we can show that,

\begin{align*}
&G\sum_{i\in\A}\left(\underbrace{ \Delta^{opt}_{\max,c_{j+1}} + \dfrac{\Delta^{opt}_{\max,c_{j+1}}\log(\psi T(\Delta^{chg}_{i,c_j})^4)}{2(\Delta^{chg}_{\epsilon_0,c_j})^2} + \Delta^{opt}_{i,c_{j+1}} + \dfrac{\Delta^{opt}_{i,c_{j+1}}\log(\psi T(\Delta^{chg}_{i,c_j})^4)}{2(\Delta^{chg}_{i,c_j})^2}}_{\textbf{part D}}\right)\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
& \leq G\sqrt{\dfrac{K^3\log K}{T}} + 8G\sqrt{KT\log K}(\log\left(\sqrt{T}\right) + G\sqrt{\dfrac{K^3\log K}{T}} + 8G\sqrt{KT\log K}(\log\left(\sqrt{T}\right)\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
& = 2G\sqrt{\dfrac{K^3\log K}{T}} + 16G\sqrt{KT\log K}\left(\log\left(\sqrt{T}\right)\right).
\end{align*}

Hence, combining the four parts we get that,

\begin{align*}
\E[R_T]&\leq 3G\sqrt{\dfrac{K^3\log K}{T}} + 24G\sqrt{KT\log K}\left(\log\left(\sqrt{T}\right)\right)\\
%%%%%%%%%%%%%%%%%%%%%%%%%
& + 16GK^2 \sqrt{(K\log K)}\left(\log\left( \sqrt{T}(\log K)\right) \right).
\end{align*}

\end{proof}


\begin{discussion}
From the result in Corollary \ref{psbandit:corollary:1}, we see that the largest contributing factor to the expected regret is of the order $O\left( G\sqrt{KT\log K}\log(\sqrt{T})\right)$.
\end{discussion}