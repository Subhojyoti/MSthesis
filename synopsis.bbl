\begin{thebibliography}{26}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{{\tt #1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Abernethy {\em et~al.\/}(2016)Abernethy, Amin, and
  Zhu}]{abernethy2016threshold}
{\bf Abernethy, J.~D.}, {\bf K.~Amin}, and {\bf R.~Zhu} (2016).
\newblock Threshold bandits, with and without censored feedback.
\newblock {\em 29th Annual Conference on Neural Information Processing Systems
  2016, December 5-10, 2016, Barcelona, Spain\/}, 4889--4897.

\bibitem[{Agrawal and Goyal(2012)}]{agrawal2012analysis}
{\bf Agrawal, S.} and {\bf N.~Goyal} (2012).
\newblock Analysis of thompson sampling for the multi-armed bandit problem.
\newblock {\em {COLT} 2012 - The 25th Annual Conference on Learning Theory,
  June 25-27, 2012, Edinburgh, Scotland\/}, 39.1--39.26.

\bibitem[{Audibert and Bubeck(2009)}]{audibert2009minimax}
{\bf Audibert, J.} and {\bf S.~Bubeck} (2009).
\newblock Minimax policies for adversarial and stochastic bandits.
\newblock {\em {COLT} 2009 - The 22nd Conference on Learning Theory, Montreal,
  Quebec, Canada, June 18-21, 2009\/}, 217--226.

\bibitem[{Audibert {\em et~al.\/}(2010)Audibert, Bubeck, and
  Munos}]{audibert2010best}
{\bf Audibert, J.}, {\bf S.~Bubeck}, and {\bf R.~Munos} (2010).
\newblock Best arm identification in multi-armed bandits.
\newblock {\em {COLT} 2010 - The 23rd Conference on Learning Theory, Haifa,
  Israel, June 27-29, 2010\/}, 41--53.

\bibitem[{Audibert {\em et~al.\/}(2009)Audibert, Munos, and
  Szepesv{\'a}ri}]{audibert2009exploration}
{\bf Audibert, J.-Y.}, {\bf R.~Munos}, and {\bf C.~Szepesv{\'a}ri} (2009).
\newblock Exploration--exploitation tradeoff using variance estimates in
  multi-armed bandits.
\newblock {\em Theoretical Computer Science\/}, {\bf 410}(19), 1876--1902.

\bibitem[{Auer {\em et~al.\/}(2002)Auer, Cesa-Bianchi, and
  Fischer}]{auer2002finite}
{\bf Auer, P.}, {\bf N.~Cesa-Bianchi}, and {\bf P.~Fischer} (2002).
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock {\em Machine learning\/}, {\bf 47}(2-3), 235--256.

\bibitem[{Auer and Ortner(2010)}]{auer2010ucb}
{\bf Auer, P.} and {\bf R.~Ortner} (2010).
\newblock Ucb revisited: Improved regret bounds for the stochastic multi-armed
  bandit problem.
\newblock {\em Periodica Mathematica Hungarica\/}, {\bf 61}(1-2), 55--65.

\bibitem[{Bertsekas and Tsitsiklis(1996)}]{bertsekas1996neuro}
{\bf Bertsekas, D.~P.} and {\bf J.~N. Tsitsiklis} (1996).
\newblock Neuro-dynamic programming (optimization and neural computation
  series, 3).
\newblock {\em Athena Scientific\/}, {\bf 7}, 15--23.

\bibitem[{Bubeck {\em et~al.\/}(2011)Bubeck, Munos, and
  Stoltz}]{bubeck2011pure}
{\bf Bubeck, S.}, {\bf R.~Munos}, and {\bf G.~Stoltz} (2011).
\newblock Pure exploration in finitely-armed and continuous-armed bandits.
\newblock {\em Theoretical Computer Science\/}, {\bf 412}(19), 1832--1852.

\bibitem[{Chen {\em et~al.\/}(2014)Chen, Lin, King, Lyu, and
  Chen}]{chen2014combinatorial}
{\bf Chen, S.}, {\bf T.~Lin}, {\bf I.~King}, {\bf M.~R. Lyu}, and {\bf W.~Chen}
  (2014).
\newblock Combinatorial pure exploration of multi-armed bandits.
\newblock {\em 27th Annual Conference on Neural Information Processing Systems
  2014, December 8-13 2014, Montreal, Quebec, Canada\/}, 379--387.

\bibitem[{Even-Dar {\em et~al.\/}(2006)Even-Dar, Mannor, and
  Mansour}]{even2006action}
{\bf Even-Dar, E.}, {\bf S.~Mannor}, and {\bf Y.~Mansour} (2006).
\newblock Action elimination and stopping conditions for the multi-armed bandit
  and reinforcement learning problems.
\newblock {\em The Journal of Machine Learning Research\/}, {\bf 7},
  1079--1105.

\bibitem[{Gabillon {\em et~al.\/}(2011)Gabillon, Ghavamzadeh, Lazaric, and
  Bubeck}]{gabillon2011multi}
{\bf Gabillon, V.}, {\bf M.~Ghavamzadeh}, {\bf A.~Lazaric}, and {\bf S.~Bubeck}
  (2011).
\newblock Multi-bandit best arm identification.
\newblock {\em 25th Annual Conference on Neural Information Processing Systems
  2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain.\/},
  2222--2230.

\bibitem[{Garivier and Capp{\'e}(2011)}]{garivier2011kl}
{\bf Garivier, A.} and {\bf O.~Capp{\'e}} (2011).
\newblock The kl-ucb algorithm for bounded stochastic bandits and beyond.
\newblock {\em arXiv preprint arXiv:1102.2490\/}.

\bibitem[{Ghavamzadeh {\em et~al.\/}(2015)Ghavamzadeh, Mannor, Pineau, Tamar
  {\em et~al.\/}}]{ghavamzadeh2015bayesian}
{\bf Ghavamzadeh, M.}, {\bf S.~Mannor}, {\bf J.~Pineau}, {\bf A.~Tamar}, {\em
  et~al.\/}, {\em Bayesian reinforcement learning: a survey\/}.
\newblock World Scientific, 2015.

\bibitem[{Honda and Takemura(2010)}]{honda2010asymptotically}
{\bf Honda, J.} and {\bf A.~Takemura} (2010).
\newblock An asymptotically optimal bandit algorithm for bounded support
  models.
\newblock {\em {COLT} 2010 - The 23rd Conference on Learning Theory, Haifa,
  Israel, June 27-29, 2010\/}, 67--79.

\bibitem[{Kaufmann {\em et~al.\/}(2012)Kaufmann, Capp{\'{e}}, and
  Garivier}]{kaufmann2012bayesian}
{\bf Kaufmann, E.}, {\bf O.~Capp{\'{e}}}, and {\bf A.~Garivier} (2012).
\newblock On bayesian upper confidence bounds for bandit problems.
\newblock {\em Proceedings of the Fifteenth International Conference on
  Artificial Intelligence and Statistics, {AISTATS} 2012, La Palma, Canary
  Islands, April 21-23, 2012\/}, {\bf 22}, 592--600.

\bibitem[{Lattimore(2015)}]{lattimore2015optimally}
{\bf Lattimore, T.} (2015).
\newblock Optimally confident ucb: Improved regret for finite-armed bandits.
\newblock {\em arXiv preprint arXiv:1507.07880\/}.

\bibitem[{Liu and Tsuruoka(2016)}]{liu2016modification}
{\bf Liu, Y.} and {\bf Y.~Tsuruoka} (2016).
\newblock Modification of improved upper confidence bounds for regulating
  exploration in monte-carlo tree search.
\newblock {\em Theoretical Computer Science\/}, {\bf 644}, 92--105.

\bibitem[{Locatelli {\em et~al.\/}(2016)Locatelli, Gutzeit, and
  Carpentier}]{locatelli2016optimal}
{\bf Locatelli, A.}, {\bf M.~Gutzeit}, and {\bf A.~Carpentier} (2016).
\newblock An optimal algorithm for the thresholding bandit problem.
\newblock {\em Proceedings of the 33nd International Conference on Machine
  Learning, {ICML} 2016, NY, USA, June 19-24, 2016\/}, {\bf 48}, 1690--1698.

\bibitem[{Maillard(2011)}]{maillard2011apprentissage}
{\bf Maillard, O.-A.} (2011).
\newblock {\em LEARNING S {\ 'E} QUENTIAL: Bandits, Statistics and
  Reinforcement.\/}.
\newblock Ph.D. thesis, University of Science and Technology of Lille-Lille I.

\bibitem[{Robbins(1952)}]{robbins1952some}
{\bf Robbins, H.}, Some aspects of the sequential design of experiments.
\newblock {\em In\/} {\em Herbert Robbins Selected Papers\/}. Springer, 1952,
  169--177.

\bibitem[{Steinwart {\em et~al.\/}(2005)Steinwart, Hush, and
  Scovel}]{steinwart2005classification}
{\bf Steinwart, I.}, {\bf D.~Hush}, and {\bf C.~Scovel} (2005).
\newblock A classification framework for anomaly detection.
\newblock {\em Journal of Machine Learning Research\/}, {\bf 6}(Feb), 211--232.

\bibitem[{Streeter and Smith(2006)}]{streeter2006selecting}
{\bf Streeter, M.~J.} and {\bf S.~F. Smith} (2006).
\newblock Selecting among heuristics by solving thresholded k-armed bandit
  problems.
\newblock {\em ICAPS 2006\/}, 123--127.

\bibitem[{Sutton and Barto(1998)}]{sutton1998reinforcement}
{\bf Sutton, R.~S.} and {\bf A.~G. Barto}, {\em Reinforcement learning: An
  introduction\/}.
\newblock MIT press, 1998.

\bibitem[{Thompson(1933)}]{thompson1933likelihood}
{\bf Thompson, W.~R.} (1933).
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock {\em Biometrika\/}, 285--294.

\bibitem[{Thompson(1935)}]{thompson1935theory}
{\bf Thompson, W.~R.} (1935).
\newblock On the theory of apportionment.
\newblock {\em American Journal of Mathematics\/}, {\bf 57}(2), 450--456.

\end{thebibliography}
