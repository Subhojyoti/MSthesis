Significant amount of literature is available on the stochastic MAB setting with respect to minimizing the cumulative regret. Chapter \ref{chap:SMAB} and \ref{chap:EUCBV} deals with that. In this work we are particularly interested in \emph{pure-exploration MABs},  where the focus in primarily on simple regret rather than the cumulative regret. The relationship between cumulative regret and simple regret is proved in \citet{bubeck2011pure} where the authors prove that minimizing the simple regret necessarily results in maximizing the cumulative regret.
The pure exploration problem has been explored  mainly under the following two settings:
	
\subsection{Fixed Budget setting} 

Here the learning algorithm has to suggest the best arm(s) within a fixed budget or time-horizon $T$, that is given as an input. The objective is to maximize the probability of returning the best arm(s).  This is the scenario we consider in this chapter. Some of the important algorithms used in pure exploration setting are discussed in the next part.

\subsubsection{UCB-Exploration Algorithm}

One of the first algorithms proposed for the fixed budget setting is the UCB-Exploration (UCBE) algorithm in \citet{audibert2010best} used for identifying a single best arm. This is shown in algorithm \ref{alg:ucbe}.


\begin{algorithm}[!ht]
\caption{UCBE}
\label{alg:ucbe}
\begin{algorithmic}[1]
\State \textbf{Input: } The budget $T$, exploration parameter $a$
\State Pull each arm once
\For{$t=K+1,..., T$}
\State Pull the arm such that $\argmax_{i\in \A}\bigg\lbrace\hat{r}_{i} + \sqrt{\dfrac{a}{n_i}}\bigg\rbrace$, where $a = \dfrac{25(T-K)}{36 H_1}$ and $H_1 = \sum_{i=1}^{K}\dfrac{1}{\Delta_i^2}$.
\State $t:=t+1 $
\EndFor
\end{algorithmic}
\end{algorithm}

This algorithm is quite similar to the UCB1 algorithm discussed in \citet{auer2002finite} (see algorithm \ref{alg:ucb1}). The major difference between the two algorithms is the confidence interval such that for UCB1 it is designed for minimizing the cumulative regret but for UCBE it is designed for minimizing simple regret. An illustrative table comparing the two is provided in table \ref{table:comp-exp}.

\begin{table}
\caption{Confidence interval and exploration parameters of different algorithms}
\label{table:comp-exp}
\begin{center}
\begin{tabular}{|p{5em}|p{6em}|p{8em}|p{12em}|}
\hline
Algorithm  &  Confidence interval & Exploration Parameter$(a)$ & Remarks \\
\hline
\hline
UCB1       & $\sqrt{\dfrac{a}{n_i}}$ & \begin{align*}a = 2\log (t)\end{align*} & $a$ is logarithmic in $t$ to minimize cumulative regret. This achieves a balance between exploration and exploitation. Hence, the cumulative regret grows logarithmically with $t$. \\%\midrule
\hline
\hline
UCBE       & $\sqrt{\dfrac{a}{n_i}}$ & \begin{align*}a = \dfrac{25(T-K)}{36 H_1},\\ \text{ where }H_1 = \sum_{i=1}^{K}\dfrac{1}{\Delta_i^2}\end{align*} & $a$ is linear in $T$ to minimize simple regret. Here, the main concern is to minimize the probability of error at the end of budget $T$ and conduct as much exploration as possible. Hence, a large $a$ helps to reach exponentially low probability of error.\\\midrule
\end{tabular}
\end{center}
\end{table}


\subsubsection{Successive Reject Algorithm}

The Successive Reject (SR) algorithm has also been proposed in \citet{audibert2010best} and is used for identifying a single best arm. This algorithm is quite different than upper confidence bound based algorithms because it does not rely on any explicit confidence interval to select arm at every timestep. It is shown in algorithm \ref{alg:sr}.


\begin{algorithm}[!th]
\caption{Successive Reject(SR)}
\label{alg:sr}
\begin{algorithmic}[1]
\State \textbf{Input: } The budget $T$
\State \textbf{Initialization: } $n_0 = 0$
\State \textbf{Definition: } $\bar{\log K} = \dfrac{1}{2} + \sum_{i=2}^{K}\dfrac{1}{i}$, $n_k = \dfrac{1}{\bar{\log K}}\dfrac{T-K}{K + 1 - m}$
\For{For each phase $m=1,..., K-1$}
\State For each $i \in B_{m}$, select arm $i$ for $n_k - n_{k-1}$ timesteps.
\State Let $B_{m+1} = B_m\setminus \argmin_{i\in B_m} \hat{r}_i$
(remove one element from $B_m$ , if there
is a tie, select randomly the arm to dismiss among the worst arms).
\State $m:=m+1 $
\EndFor
\State Output the single remaining $i\in B_{m}$.
\end{algorithmic}
\end{algorithm}


From algorithm \ref{alg:sr} we see that SR is a round based algorithm quite similar to UCB-Improved (see algorithm \ref{alg:ucbi}). Similar to UCB-Improved, SR pulls all arms equal number of times in each round and then discards some arm that it deems to be sub-optimal until it is left with a single best arm. However, SR does not have any explicit confidence interval as UCBE, rather the idea of the confidence interval is hidden in the number of pulls allocated to each arm in every round. The number of times each arm is pulled in every round, that is $n_k - n_{k-1}$ timesteps makes sure that the optimal arm is not eliminated in the $k$-th round with high probability. 

In the combinatorial fixed budget setup \citet{gabillon2011multi} propose the GapE and GapE-V algorithms that suggest, with high probability, the best $m$ arms at the end of the time budget. 

Similarly, \citet{bubeck2013multiple} introduce the  Successive Accept Reject (SAR) algorithm, which is an extension of the SR algorithm; SAR is a round based algorithm whereby at the end of each round an arm is either accepted or rejected (based on certain confidence conditions) until the top $m$ arms are suggested at the end of the budget with high probability. A similar combinatorial setup was explored in \citet{chen2014combinatorial} where the authors propose the Combinatorial Successive Accept Reject (CSAR) algorithm, which is similar in concept to SAR but with a more general setup. 

\subsection{Fixed Confidence setting} 

In this setting the learning algorithm has to suggest the best arm(s) with a fixed confidence (given as input) with as fewer number of attempts as possible. The single best arm identification has been studied in \citet{even2006action}, while for the combinatorial setup \citet{kalyanakrishnan2012pac} have proposed the LUCB algorithm which, on termination, returns  $m$ arms which are at least $\epsilon$ close to the true top-$m$ arms with probability at least $1-\delta$. For a detail survey of this setup we refer the reader to \citet{jamieson2014best}. 

\subsection{Unified Setting}
Apart from these two settings some unified approaches has also been suggested in \citet{gabillon2012best} which proposes the algorithms UGapEb and UGapEc which can work in both the above two settings. The thresholding bandit problem is a specific instance of the pure-exploration setup of \citet{chen2014combinatorial}. 



	
	
