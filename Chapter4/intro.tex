In the previous chapters \ref{chap:SMAB} and \ref{chap:EUCBV} we studied the stochastic multi-armed bandit (SMAB) setting with the goal of minimizing cumulative regret. In this chapter we will study another setting called Pure-exploration multi-armed bandits. An interested reader can read through the previous chapters or can continue from here. Though we re-use the ideas from SMABs, the goal of pure exploration setup is distinctly different from that of cumulative regret minimization of SMABs and the required algorithms to understand this setup are mentioned in this chapter itself. Pure-exploration MAB problems are unlike their traditional (exploration vs.\ exploitation)  counterparts, the SMABs, where the  objective is to minimize the cumulative regret. The cumulative regret is the total loss incurred by the learner for not playing the optimal arm throughout the time horizon $T$. 
	In pure-exploration problems a learning algorithm, until time $T$, can invest entirely on exploring the arms without being concerned about the loss incurred while exploring; the objective is to minimize the probability that the arm recommended at time $T$ is not the best arm.  In this chapter, we further consider a combinatorial version of the pure-exploration MAB, called the thresholding bandit problem (TBP).  Here, the learning algorithm is provided with a threshold $\tau$, and the objective, after exploring for $T$ rounds, is to  output all arms $i$ whose $r_{i}$ is above $\tau$. 
It is important to emphasize that the \emph{thresholding} bandit problem is different from the \emph{threshold} bandit setup studied in \cite{abernethy2016threshold}, where the learner receives an unit reward whenever the value of an observation is above a threshold. 

	The rest of the chapter is organized as follows. We specify all the notations and assumptions in section~\ref{tbandit:notations}. Then we define the problem statement for the TBP setting in section~\ref{tbandit:probDef}. In the next section~\ref{tbandit:motivation} we discuss the motivations behind the TBP setting. In section~\ref{tbandit:prevRes} we discuss extensively on the various state-of-the-art algorithms available for the pure exploration setting and then in section~\ref{tbandit:prevResAPT} we discuss the latest works done in the TBP setting. Finally, we draw our conclusions in section \ref{tbandit:conc}.

