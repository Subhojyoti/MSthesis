In this section, we conduct extensive empirical evaluations of EUCBV against several other popular MAB  algorithms. We use expected cumulative regret as the metric of comparison. The comparison is conducted against the following algorithms: KLUCB+ \citep{garivier2011kl}, DMED \citep{honda2010asymptotically}, MOSS \citep{audibert2009minimax}, UCB1 \citep{auer2002finite}, UCB-Improved \citep{auer2010ucb}, Median Elimination \citep{even2006action}, Thompson Sampling (TS) \citep{agrawal2011analysis}, OCUCB \citep{lattimore2015optimally}, Bayes-UCB (BU) \citep{kaufmann2012bayesian} and UCB-V \citep{audibert2009exploration}\footnote{The implementation for KLUCB, Bayes-UCB and DMED were taken from \citet{CapGarKau12}}. The parameters of EUCBV algorithm for all the experiments are set as follows: $\psi=\frac{T}{K^2}$ and $\rho =0.5$ (as in Corollary \ref{Result:Corollary:1}). Note that KLUCB+ empirically outperforms KLUCB (as shown in \citet{garivier2011kl}).

\begin{figure}[!th]
    \centering
    \begin{tabular}{cc}
    \setlength{\tabcolsep}{0.1pt}
    \subfigure[0.25\textwidth][Expt-$1$: $20$ Bernoulli-distributed arms ]
    %with $r_{i_{{i}\neq {*}}}=0.07$ and $r^{*}=0.1$
    {
    		\pgfplotsset{
		tick label style={font=\Large},
		label style={font=\Large},
		legend style={font=\Large},
		ylabel style={yshift=5pt},
		%legend style={legendshift=32pt},
		}
        \begin{tikzpicture}[scale=0.7]
      	\begin{axis}[
		xlabel={timestep},
		ylabel={Cumulative Regret},
		grid=major,
        %clip mode=individual,grid,grid style={gray!30},
        clip=true,
        %clip mode=individual,grid,grid style={gray!30},
  		legend style={at={(0.5,1.5)},anchor=north, legend columns=3} ]
      	% UCB
		\addplot table{Chapter3/results/NewExpt/Expt1/UCBV01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt1/EUCBV01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt1/KLUCB01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt1/MOSS01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt1/DMED01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt1/UCB01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt1/TS01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt1/OCUCB01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt1/BU01_comp_subsampled.txt};
      	\legend{UCB-V,EUCBV,KLUCB+,MOSS,DMED,UCB1,TS,OCUCB,BU}      	
      	\end{axis}
      	\end{tikzpicture}
  		\label{fig:1}
    }
    &
    \subfigure[0.25\textwidth][Expt-$2$: $3$ Group Mean Setting ]
    %with $r_{i_{{i}\neq {*}:1-33}}=0.1$, $r_{i_{{i}\neq {*}:34-99}}=0.6$, $r^{*}_{i=100}=0.9$ and $\sigma_{i=1:100}^{2} = 0.3$
    {
    		\pgfplotsset{
		tick label style={font=\Large},
		label style={font=\Large},
		legend style={font=\Large},
		ylabel style={yshift=5pt},
		}
        \begin{tikzpicture}[scale=0.7]
        \begin{axis}[
		xlabel={timestep},
		ylabel={Cumulative Regret},
        %clip mode=individual,grid,grid style={gray!30},
       	grid=major,
       	clip=true,
  		legend style={at={(0.5,1.5)},anchor=north, legend columns=3} ]
      	% UCB
		\addplot table{Chapter3/results/NewExpt/Expt2/UCBV01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt2/EUCBV01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt2/KLUCB01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt2/MOSS01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt2/UCBR01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt2/UCB01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt2/TS01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt2/OCUCB01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt2/BU01_comp_subsampled.txt};      	
      	\legend{UCB-V,EUCBV,KLUCB-G+,MOSS,UCB-Imp,UCB1,TS-G,OCUCB,BU-G}
      	\end{axis}
      	\end{tikzpicture}
   		\label{fig:2}
    }
    \end{tabular}
    \caption{A comparison of the cumulative regret incurred by the various bandit algorithms. }
    \label{fig:karmed}
    \vspace*{-1em}
\end{figure}
% For the purpose of performance comparison


\textbf{Experiment-1 (Bernoulli with uniform gaps):} This experiment is conducted to observe the performance of EUCBV over a short horizon. The horizon $T$ is set to $60000$. The testbed comprises of $20$ Bernoulli distributed arms with expected rewards of the arms as $r_{1:19}=0.07$ and $r^{*}_{20}=0.1$ and these type of cases are frequently encountered in web-advertising domain (see \cite{garivier2011kl}). The regret is averaged over $100$ independent runs and is shown in Figure \ref{fig:1}. EUCBV, MOSS, OCUCB, UCB1, UCB-V, KLUCB+, TS, BU and DMED are run in this experimental setup. Not only do we observe that EUCBV performs better than all the non-variance based algorithms such as MOSS, OCUCB, UCB-Improved and UCB1, but it also outperforms UCBV because of the choice of the exploration parameters. Because of the small gaps and short horizon $T$, we do not compare with UCB-Improved and Median Elimination for this test-case. 

\textbf{Experiment-2 (Gaussian $3$ Group Mean Setting):} This experiment is conducted to observe the performance of EUCBV over a large horizon in Gaussian distribution testbed. This setting comprises of a large horizon of $T = 3\times 10^{5}$ timesteps and a large set of arms. This testbed comprises of $100$ arms involving Gaussian reward distributions with expected rewards of the arms in $3$ groups, $r_{1:66}=0.07$, $r_{67:99}=0.01$ and $r^{*}_{100}=0.09$ with variance set as $\sigma_{1:66}^{2} = 0.01,\sigma_{67:99}^{2} = 0.25$ and $\sigma^{2}_{100}=0.25$. The regret is averaged over $100$ independent runs and is shown in Figure \ref{fig:2}. From the results in Figure \ref{fig:2}, we observe that since the gaps are small and the variances of the optimal arm and the arms farthest from the optimal arm are the highest, EUCBV, which allocates pulls proportional to the variances of the arms, outperforms all the non-variance based algorithms MOSS, OCUCB, UCB1, UCB-Improved and Median-Elimination ($\epsilon=0.1,\delta=0.1$). The performance of Median-Elimination is extremely weak in comparison with the other algorithms and its plot is not shown in Figure \ref{fig:2}. We omit its plot in order to more clearly show the difference between EUCBV, MOSS and OCUCB. Also note that the order of magnitude in the y-axis (cumulative regret) of Figure \ref{fig:2} is $10^4$. KLUCB-Gauss+ (denoted by KLUCB-G+), TS-G and BU-G are initialized with Gaussian priors. Both KLUCB-G+ and UCBV which is a variance-aware algorithm perform much worse than TS-G and EUCBV. The performance of DMED is similar to KLUCB-G+ in this setup and its plot is omitted. 


\begin{figure}[!h]
    \centering
    \begin{tabular}{cc}
    \subfigure[0.25\textwidth][Expt-$3$: Failure of TS]
    {
    		\pgfplotsset{
		tick label style={font=\Large},
		label style={font=\Large},
		legend style={font=\Large},
		ylabel style={yshift=5pt},
		}
        \begin{tikzpicture}[scale=0.7]
      	\begin{axis}[
		ylabel={Cumulative Regret},
		xlabel={timestep},
		grid=major,
        %clip mode=individual,grid,grid style={gray!30},
        clip=true,
        %clip mode=individual,grid,grid style={gray!30},
  		legend style={at={(0.5,1.3)},anchor=north, legend columns=3} ]
      	% UCB
		\addplot table{Chapter3/results/NewExpt/Expt3/UCBV01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt3/EUCBV01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt3/MOSS01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt3/TS01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt3/OCUCB01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt3/BU01_comp_subsampled.txt};
      	\legend{UCBV,EUCBV,MOSS,TS-G,OCUCB,BU-G} 
      	\end{axis}
      	\end{tikzpicture}
  		\label{fig:3}
    }
    &
    \subfigure[0.25\textwidth][Expt-$4$: $3$ Group Variance Setting]
    %with $r_{i_{{i}\neq {*}}}=0.05$ and $r^{*}=0.1$
    {
    	\pgfplotsset{
		tick label style={font=\Large},
		label style={font=\Large},
		legend style={font=\Large},
		ylabel style={yshift=5pt},
		}
        \begin{tikzpicture}[scale=0.7]
        \begin{axis}[
		xlabel={timestep},
		ylabel={Cumulative Regret},
        %clip mode=individual,grid,grid style={gray!30},
		grid=major,
		clip=true,
  		legend style={at={(0.5,1.3)},anchor=north, legend columns=3} ]
        % UCB
		\addplot table{Chapter3/results/NewExpt/Expt41/UCBV01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt41/EUCBV01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt41/MOSS01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt41/TS01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt41/OCUCB01_comp_subsampled.txt};
		\addplot table{Chapter3/results/NewExpt/Expt41/BU01_comp_subsampled.txt};
      	\legend{UCBV,EUCBV,MOSS,TS-G,OCUCB,BU-G} 
      	\end{axis}
        \end{tikzpicture}
        \label{fig:4}
    }
	\end{tabular}
	\label{fig:furtherExpt1}
    \caption{Further Experiments with EUCBV}
    \vspace*{-1em}
\end{figure}
%\vspace*{-0.5em}



\textbf{Experiment-3 (Failure of TS):} This experiment is conducted to demonstrate that in certain environments when the horizon is large, gaps are small and the variance of the optimal arm is high, the Bayesian algorithms (like TS) do not perform well but EUCBV performs exceptionally well. This experiment is conducted on $100$ Gaussian distributed arms such that expected rewards of the arms $r_{1:10}=0.045$, $r_{11:99}=0.04$, $r^{*}_{100}=0.05$ and the variance is set as $\sigma_{1:10}^{2}=0.01$,   $\sigma_{100}^{2}=0.25$ and $T=4\times 10^5$. The variance of the arms $i=11:99$ are chosen uniform randomly between $[0.2,0.24]$. TS and BU with Gaussian priors fail because here the chosen variance values are such that only variance-aware algorithms with appropriate exploration factors will perform  well or otherwise it will get bogged down in costly exploration. The algorithms that are not variance-aware will spend a significant amount of pulls trying to find the optimal arm. The result is shown in Figure \ref{fig:3}. Predictably EUCBV, which allocates pulls proportional to the variance of the arms, outperforms its closest competitors TS-G, BU-G, UCBV, MOSS and OCUCB. The plots for KLUCB-G+, DMED, UCB1, UCB-Improved and Median Elimination are omitted from the figure as their performance is extremely weak in comparison with other algorithms. We omit their plots to clearly show how EUCBV outperforms its nearest competitors. Note that EUCBV by virtue of its aggressive exploration parameters outperforms UCBV in all the experiments even though UCBV is a variance-based algorithm. The performance of TS-G is also weak and this is in line with the observation in \citet{lattimore2015optimally} that the worst case regret of TS when Gaussian prior is used is $\Omega\left( \sqrt{KT\log T}\right)$.



\textbf{Experiment-4 (Gaussian $3$ Group Variance setting):} This experiment is conducted to show that when the gaps are uniform and variance of the arms are the only discriminative factor then the EUCBV performs extremely well over a very large horizon and over a large number of arms. This testbed comprises of $100$ arms with Gaussian reward distributions, where the expected rewards of the arms are $r_{1:99}=0.09$ and $r^{*}_{100}=0.1$. The variances of the arms are divided into $3$ groups. The group $1$ consist of arms $i=1:49$ where the variances are chosen uniform randomly between $[0.0,0.05]$, group $2$ consist of arms $i=50:99$ where the variances are chosen uniform randomly between $[0.19,0.24]$ and for the optimal arm $i=100$ (group $3$) the variance is set as $\sigma_{*}^{2}=0.25$. We report the cumulative regret averaged over $100$ independent runs. The horizon is set at $T=4\times 10^{5}$ timesteps. We report the performance of MOSS,BU-G, UCBV, TS-G and OCUCB who are the closest competitors of EUCBV over this uniform gap setup. From the results in Figure \ref{fig:4}, it is evident that the growth of regret for EUCBV  is much lower than that of TS-G, MOSS, BU-G, OCUCB and UCBV. Because of the poor performance of KLUCB-G+ in the last two experiments we do not implement it in this setup. Also, note that for optimal performance BU-G, TS-G and KLUCB-G+ require the knowledge of the type of distribution to set their priors . Also, in all the experiments with Gaussian distributions EUCBV significantly outperforms all the Bayesian algorithms initialized with Gaussian priors.


