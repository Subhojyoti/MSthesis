The main result of this chapter is presented in the following theorem, where we establish a regret upper bound for the proposed EUCBV  algorithm. 
% \subsection*{Main Theorem}

\subsubsection{Gap-Dependent bound of EUCBV}

\begin{theorem}[\textbf{\textit{Gap-Dependent Bound}}]
\label{Result:Theorem:1}
For $T\geq K^{2.4}$, $\rho=\frac{1}{2}$ and $\psi=\frac{T}{K^2}$, the regret $R_T$ for EUCBV satisfies
\begin{align*}
\E [R_{T}] \leq &\sum\limits_{i\in \A :\Delta_{i} > b}\bigg\lbrace \dfrac{C_0 K^{4}}{T^{\frac{1}{4}}} + \bigg(\Delta_{i}+\dfrac{320\sigma_i^2\log{(\frac{T\Delta_{i}^{2}}{K})}}{\Delta_{i}}\bigg)\bigg \rbrace\\ 
  & +\sum\limits_{i\in \A :0 < \Delta_{i}\leq b} \dfrac{C_2 K^{4}}{T^{\frac{1}{4}}} + \max_{i\in \A :0 < \Delta_{i}\leq b}\Delta_{i}T.
\end{align*}

for all $b\geq\sqrt{\frac{e}{T}}$ and $C_0, C_2$ are integer constants. 
\end{theorem}

\begin{proof}[Outline]
The proof is along the lines of the technique in \citet{auer2010ucb}. It comprises of three modules. In the first module we prove the necessary conditions for arm elimination within a specified number of rounds. However, here we require some additional technical results (see Lemma~\ref{proofTheorem:Lemma:1} and Lemma~\ref{proofTheorem:Lemma:2}) to bound the length of the confidence intervals. Further, note that our algorithm combines the variance-estimate based approach of \citet{audibert2009exploration} with the arm-elimination technique of \citet{auer2010ucb} (see Lemma~\ref{proofTheorem:Lemma:3}). Also, while \citet{auer2010ucb} uses Chernoff-Hoeffding bound (see \ref{app:chernoff}) to derive their regret bound whereas in our work we use  Bernstein inequality (as in \citet{audibert2009exploration}, see \ref{app:bernstein}) to obtain the bound. To bound the probability of the non-uniform arm selection before it gets eliminated we use Lemma~\ref{proofTheorem:Lemma:4} and Lemma~\ref{proofTheorem:Lemma:5}. In the second module we bound the number of pulls required if an arm is eliminated on or before a particular number of rounds. Note that the number of pulls allocated in a round $m$ for each arm is $n_{m}:=\bigg\lceil\frac{\log{(\psi T\epsilon_{m}^{2})}}{2\epsilon_{m}}\bigg\rceil$ which is much lower than the number of pulls of each arm required by UCB-Improved or Median-Elimination. We introduce the variance term in the most significant term in the bound by Lemma~\ref{proofTheorem:Lemma:6}. Finally, the third module deals with case of bounding the regret, given that a sub-optimal arm eliminates the optimal arm.
% (see Lemma~\ref{proofTheorem:Lemma:9}). The detailed proof is available in Section \ref{sec:proofTheorem:Theorem1}.
\hfill $\blacksquare$
\end{proof}

\begin{discussion}
\label{Result:discussion1}
From the above result we see that the most significant term in the gap-dependent bound is of the order $O\left(\frac{K\sigma^2_{\max}\log{(T\Delta^{2}/K)}}{\Delta}\right)$ which is better than the existing results for UCB1, UCBV, MOSS and UCB-Improved (see Table~\ref{tab:comp-bds}). Also as like UCBV, this term scales with the variance. \citet{audibert2010best} have defined the term $H_1=\sum_{i=1}^{K}\frac{1}{\Delta_i^2}$, which is referred to as the hardness of a problem; \citet{bubeck2012regret} have conjectured that the gap-dependent regret upper bound can match $O\left(\frac{K\log{(T/H_1)}}{\Delta}\right)$. However, in  \citet{lattimore2015optimally} it is proved that the gap-dependent regret bound cannot be lower than $O\left(\sum_{i=2}^{K}\frac{\log\left(T/H_i\right)}{\Delta_i}\right)$, where $H_i=\sum_{j=1}^{K}\min\left\lbrace \frac{1}{\Delta_i^2},\frac{1}{\Delta_j^2}\right\rbrace$ (OCUCB proposed in \citet{lattimore2015optimally} achieves this bound). Further, in \citet{lattimore2015optimally} it is shown that only in the worst case scenario when all the gaps are equal (so that $H_1=H_{i}=\sum_{i=1}^{K}\frac{1}{\Delta^2}$) the above two bounds match. In the latter scenario, considering $\sigma^2_{\max}\leq \frac{1}{4}$ as all rewards are bounded in $[0,1]$, we see that the gap-dependent bound of EUCBV simplifies to $O\left(\frac{K\log{(T/H_1)}}{\Delta}\right)$, thus matching the gap-dependent bound of OCUCB which is order optimal.
\end{discussion}


\subsubsection{Gap-Independent bound of EUCBV}

In this section, we specialize the result of Theorem \ref{Result:Theorem:1} in Corollary \ref{Result:Corollary:1} to  obtain the gap-independent worst case regret bound. %and Corollary \ref{Result:Corollary:2}.


%\subsection*{Corollary 1}

\begin{corollary}[\textbf{\textit{Gap-Independent Bound}}]
\label{Result:Corollary:1}
When the gaps of all the sub-optimal arms are identical, i.e., $\Delta_i =\Delta = \sqrt{\frac{K\log K}{T}}>\sqrt{\frac{e}{T}}, \forall i\in \A$ and $C_3$ being an integer constant, the
regret of EUCBV is upper bounded by the following gap-independent expression:
\begin{align*}
	\E[R_{T}]\leq  \dfrac{C_3 K^5}{T^{\frac{1}{4}}} + 80\sqrt{KT}.
\end{align*}	
\end{corollary}
	
%\begin{proof}
%The proof is given in Appendix \ref{App:Corollary:1}.
%\end{proof}

\begin{proof}
\label{Proof:Corollary:1}
From \cite{bubeck2011pure}  we know that the function $x\in [0,1]\mapsto x\exp(-Cx^2)$ is  decreasing on $\left[\frac{1}{\sqrt{2C}},1\right ]$ for any $C>0$. Thus, we take $C=\left\lfloor \frac{T}{e}\right\rfloor$ and choose  $\Delta_{i}=\Delta=\sqrt{\frac{K\log K}{T}}>\sqrt{\frac{e}{T}}$ for all $i$.

First, let us recall the result in Theorem \ref{Result:Theorem:1} below:
\begin{align*}
\E [R_{T}] \leq &\sum\limits_{i\in \A :\Delta_{i} > b}\bigg\lbrace \dfrac{C_0 K^{4}}{T^{\frac{1}{4}}} + \bigg(\Delta_{i}+\dfrac{320\sigma_i^2\log{(\frac{T\Delta_{i}^{2}}{K})}}{\Delta_{i}}\bigg)\bigg \rbrace\\ 
  & +\sum\limits_{i\in \A :0 < \Delta_{i}\leq b} \dfrac{C_2 K^{4}}{T^{\frac{1}{4}}} + \max_{i\in \A :0 < \Delta_{i}\leq b}\Delta_{i}T.
\end{align*}

Now,  with  $\Delta_i =\Delta = \sqrt{\frac{K\log K}{T}}>\sqrt{\frac{e}{T}}$ we obtain,
	\begin{align*}
	\sum_{i\in \A :\Delta_{i} > b}\dfrac{320\sigma_i^2\log{(\frac{T\Delta_{i}^{2}}{K})}}{\Delta_{i}} &\leq  \dfrac{320\sigma_{\max}^2 K\sqrt{T}\log{(T\dfrac{K(\log K)}{T K})}}{\sqrt{K\log K}}\\ 
	&\leq  \dfrac{320\sigma_{\max}^2\sqrt{KT}\log{(\log K)}}{\sqrt{\log K}}\\
	&\overset{(a)}{\leq} 320\sigma_{\max}^2\sqrt{KT} 
	\end{align*}		
	where $(a)$ follows from the identity $\dfrac{\log{(\log K)}}{\sqrt{\log K}}\leq 1$ for $K\geq 2$. 	
%For the term $\sum\limits_{i\in \A :\Delta_{i} > b}\dfrac{C_0 K^{\frac{5}{2}}}{\sqrt{T\Delta_i}}$ by substituting the value of $\Delta_i=\Delta=\sqrt{\dfrac{K\log K}{T}}$ we get,
%\begin{align*}
%\dfrac{C_0 K^{\frac{5}{2}+1}}{\sqrt{T\Delta_i}} &\leq \dfrac{C_0 K^{\frac{5}{2}}}{\sqrt{T\sqrt{\dfrac{K\log K}{T}}}} \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\leq \dfrac{C_0 K^{\frac{5}{2}+1}}{(KT\log K)^{\frac{1}{4}}} \leq \dfrac{C_0 K^3}{T^{\frac{1}{4}}}
%\end{align*}	
%	
%Similarly for the term $\sum\limits_{i\in \A :0 < \Delta_{i}\leq b} \dfrac{C_2^{'} K^{\frac{5}{2}}}{\sqrt{T\Delta_i}}$ we can show that,
%\begin{align*}
%\sum\limits_{i\in \A :0 < \Delta_{i}\leq b} \dfrac{C_2^{'} K^{\frac{5}{2}}}{\sqrt{T\Delta_i}} &\leq \dfrac{C_2^{'} K^{\frac{5}{2}+1}}{\sqrt{T\sqrt{\dfrac{e}{T}}}} \leq \dfrac{C_2^{'} K^3}{T^{\frac{1}{4}}} 
%\end{align*}
Thus, the total worst case gap-independent bound is given by
	\begin{align*}
	\E[R_{T}] &\overset{(a)}{\leq}  \dfrac{C_3 K^5}{T^{\frac{1}{4}}} + 320\sigma_{\max}^2\sqrt{KT}
	\overset{(b)}{\leq} \dfrac{C_3 K^5}{T^{\frac{1}{4}}} + 80\sqrt{KT}
	\end{align*}	
	
where, in$(a)$, $C_3$ is an integer constant such that $C_3 = C_0 + C_2 $ and $(b)$ occurs because $\sigma_i^2 \in [0,\frac{1}{4}], \forall i\in \A$.

\hfill $\blacksquare$	
\end{proof}


\begin{discussion}
\label{Result:discussion2}
 In the non-stochastic scenario, \citet{auer2002nonstochastic} showed that the bound on the cumulative regret for EXP-4 is $O\left(\sqrt{KT\log K}\right)$. However, in the stochastic case, UCB1 proposed in \citet{auer2002finite} incurred a regret of order of  $O\left(\sqrt{KT\log T}\right)$ which is clearly improvable. From the above result we see that in the gap-independent bound of EUCBV the most significant term is $O\left(\sqrt{KT}\right)$ which  matches the upper bound of MOSS and OCUCB, and is better than UCB-Improved, UCB1 and UCBV (see Table~\ref{tab:comp-bds}).
\end{discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Shifted to Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{proof}
%\label{Proof:Corollary:1}
%From \cite{bubeck2011pure}  we know that the function $x\in [0,1]\mapsto x\exp(-Cx^2)$ is  decreasing on $\left[\frac{1}{\sqrt{2C}},1\right ]$ for any $C>0$. Thus, we take $C=\left\lfloor \frac{T}{e}\right\rfloor$ and choose  $\Delta_{i}=\Delta=\sqrt{\frac{K\log K}{T}}>\sqrt{\frac{e}{T}}$ for all $i$.
%
%First, let us recall the result in Theorem \ref{Result:Theorem:1} below:
%\begin{align*}
%\E [R_{T}] \leq &\sum\limits_{i\in \A :\Delta_{i} > b}\bigg\lbrace 64 K + \bigg(\Delta_{i}+\dfrac{64\log{(\frac{T\Delta_{i}^{2}}{K})}}{\Delta_{i}}\bigg)\bigg \rbrace\\ 
%  & +\sum\limits_{i\in \A :0 < \Delta_{i}\leq b} 32 K + \max_{i\in \A :0 < \Delta_{i}\leq b}\Delta_{i}T  
%\end{align*}
%
%Now,  with  $\Delta_i =\Delta = \sqrt{\frac{K\log K}{T}}>\sqrt{\frac{e}{T}}$ we obtain,
%	\begin{align*}
%	&\sum_{i\in \A :\Delta_{i} > b}\dfrac{64\log{(\frac{T\Delta_{i}^{2}}{K})}}{\Delta_{i}} \leq  \dfrac{64K\sqrt{T}\log{(T\dfrac{K(\log K)}{T K})}}{\sqrt{K\log K}}\\ 
%	&\leq  \dfrac{64\sqrt{KT}\log{(\log K)}}{\sqrt{\log K}}
%	\overset{(a)}{\leq} 64\sqrt{KT} 
%	\end{align*}		
%	where $(a)$ follows from the identity $\dfrac{\log{(\log K)}}{\sqrt{\log K}}\leq 1$ for $K\geq 2$. Thus, the total worst case gap-independent bound is given by
%	\begin{align*}
%	\E[R_{T}]\leq 96 K^2 + 64\sqrt{KT}.
%	\end{align*}	
%\hfill $\blacksquare$	
%\end{proof}
