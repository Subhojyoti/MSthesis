In this section we discuss on the various types of bandits that are available in literature. 


\subsection{Types of Bandits based on Environment}


\subsubsection{Stochastic Bandits}
In stochastic bandits, the distribution associated with each of the arms remains fixed throughout the time horizon $T$. Some of the notable papers associated with this type of setup are \citet{robbins1952some}, \citet{lai1985asymptotically},  \citet{agrawal1995sample}, \citet{auer2002finite}, \citet{auer2010ucb}, \citet{audibert2009minimax}, \citet{lattimore2015optimally}, etc. Chapter \ref{chap:SMAB} and Chapter \ref{chap:EUCBV} is based on this setup where we discuss extensively on the latest state-of-the-art algorithms.



\subsubsection{Non-stochastic Bandits}

In non-stochastic setting the distribution associated with each arm varies over the duration of the play. Two natoable examples of this are:-

\begin{enumerate}
\item \textbf{Adversarial bandits: } In adversarial bandits, an adversary decides the payoff for each arm before the learner selects an arm. This adversary may or may not be oblivious to the learning algorithm employed by the learner. In each of these cases a different guarantee on the performance of the learner can be arrived at. Some of the important papers in this setting are \citet{auer2002nonstochastic}, \citet{auer2002using}, \citet{kocak2014efficient}.

\item \textbf{Piece-wise stationary:} Another setup under this setting can be the piece-wise stationary setting. In this setting, the distribution associated with each arm is not fixed throughout the time horizon and changes either arbitrarily at particular changepoints, or changes at a fixed period. The distribution associated with each arm then  remains fixed till the next chnagepoint is encountered. Several recent works have focussed on this such as  \citep{garivier2011upper}, \citep{mellor2013thompson}, and \citep{allesiardo2017non}. In our thesis, chapter \ref{chap:psbandit} is based on this setting. 
\end{enumerate}


\subsubsection{Contextual Bandits}

The idea of clustering has been extensively studied in the contextual bandit setup, an extension of the MAB where side information or features are attached to each arm. The clustering is done over the features representing the arms to capture the complexity of the problem better when a large-number of arms are involved. Typical examples of this setting are in web-advertising domain, news article selection, etc. Some notable papers available for this setting are   \citet{auer2002using}, \citet{langford2008epoch}, \citet{li2010contextual}, \citet{beygelzimer2011contextual}, \citet{slivkins2014contextual},etc. 


%

%Clustering has been extensively studied in the area of contextual MAB. In contextual MAB, there are side-information or features attached to each arm (see  \citet{auer2002using,langford2008epoch,li2010contextual,beygelzimer2011contextual, slivkins2014contextual}).   \cite{bui2012clustered,cesa2013gang,gentile2014online}. Please note that we do not cluster over the context rather we cluster the arms into groups.




\subsection{Types of Bandits based on goal}

In bandit literature, based on the goal we can divide bandits into several categories. To illustrate his we put forward a simple scenario let us consider a stochastic bandit scenario where there are $K$ arms labeled $i=1,2,\ldots,K$ with their expected means of reward distributions ($D_i$) be denoted by $r_i$. Also let there be single optimal arm $*$ such that $r^* = \max_{i\in\A}r_i$. 



\subsubsection{Cumulative regret minimization}
In cumulative regret minimization the goal of the bandit is to minimize the cumulative regret which is the total loss suffered by the learner throughout the time horizon $T$ for not choosing the optimal arm. Formally, we can define the cumulative regret as,

\begin{eqnarray}
R_{T} = \sum_{t=1}^{T}r^* - \sum_{i\neq *}r_{i}n_{i,T} \label{eqn:chap1:regret}
\end{eqnarray}

where, $n_{i,T}$ is the number of times the learner has chosen arm $i$ over the entire horizon $T$. We can further reduce equation \ref{eqn:chap1:regret} to obtain,

\begin{align*}
R_{T} = \sum_{t=1}^{T}r^* - \sum_{i\neq *}r_{i}n_{i,T} = \sum_{i=1}^{K}\Delta_{i}n_{i,T}
\end{align*}

where $\Delta_{i}=r^* - r_i$ is called the gap between the optimal and the sub-optimal arm.

\subsubsection{Simple regret minimization}
In simple regret minimization the goal of the bandit is to minimize the instantaneous regret that is suffered at any  timestep by the learner. Formally, the simple regret at $t$-th timestep where $J_n\in\A$ is the recommendation by the learner at timestep $t$ is defined,

\begin{align*}
SR_{t} = r^* - r_{J_{n}} = \Delta_{J_n}
\end{align*}

where, $\Delta_{J_n}$ is the instantaneous gap between the expected mean of he optimal arm and the recommended arm by the learner. In the pure exploration setting the learner tries to minimize the simple regret and we study a very similar setting in chapter \ref{chap:tbandit1} and chapter \ref{chap:tbandit2}. 

\subsubsection{Weak Regret minimization}
In the non-stochastic scenario, when the distribution associated with each arm changes, the notion of regret is defined differently than cumulative regret. In this scenario, considering that there is a single best arm, the learner is more interested in minimizing the worst-case regret. Formally, for any sequence of actions $\left( j_1, \ldots , j_T \right)$ chosen by the learner over the time horizon $T$, the weak regret for single best action is defined as the difference between,
\begin{align*}
G_{\max}(j_1,\ldots,j_T) - G_{\pi}(T)
\end{align*}
where, $G_{\max}(j_1,\ldots,j_T) = \max_{i\in\A}\sum_{t=1}^{T}X_{i_t}$ is the return of the globally best action over the entire horizon $T$, $X_{i_t}$ is the reward observed for the $i$-th arm at the $t$-th timestep and $G_{\pi}(T)$ is the return following the policy $\pi$ over the horizon $T$ instead of choosing $j_1,\ldots,j_T$.

\subsection{Contextual Bandits}

Another interesting variation of the MAB model is the contextual bandit setup, where there are contexts or features associated with each arm. We can envision this with an example of online news article recommendation where there are users and articles and the goal of the learner is to map the correct article to an user so as to generate the user's  interest. Following a similar work in \citet{langford2008epoch} this problem can be formulated as a contextual MAB problem such that at every timestep $t=1,\ldots,T$
\begin{enumerate}
\item The learner observes an user $u_t$ and the set of arms (articles) $i\in\A$ along with their feature vectors $\mathcal{v}_{i,t},\forall i\in\A_t$. This vector contains information of both the users and the arms and is referred as the context.
\item On the basis of previous trials the learner pulls an arm $i_t\in\A$ at the $t$-th timestep and observes the reward $X_{i,t}$ for only the arm $i_t\in\A$.
\item The algorithm then improves its prediction for the next trial with the new observation, $\left(v_{i,t},i_{t}, X_{i,t} \right)$.
\end{enumerate}
This type of settings have been extensively studied in \citet{li2010contextual} and \citet{beygelzimer2011contextual}.

\subsection{Collaborative Bandits}

Distributed bandits is a special setting where a network of bandits collaborates with each other to identify the best set of arms. The contextual MAB model discussed before naturally extends into this setting where a network of bandits try to map articles to a large number of users by collaborating between themselves (see \citet{awerbuch2008competitive,liu2010distributed,szorenyi2013gossip,hillel2013distributed}). In this setting, bandits at the end of specific phases share information synchronously or asynchronously amongst each other to identify the best set of arms. Further, to learn more complicated structures and interaction between the user and article feature vectors, clustering can be used to cluster the articles and users based on their features and this has been studied in \citet{bui2012clustered}, \citet{cesa2013gang}, \citet{gentile2014online}.

\subsection{Bandits with Corrupt Feedback}


\subsection{Conservative Bandits}