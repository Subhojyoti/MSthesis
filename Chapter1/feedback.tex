In an online sequential setting, the feedback that the learner receives from the environment can be characterized into three broad categories, full information feedback, partial information feedback and bandit feedback. 


	To illustrate the different types of feedback we will take help of the following example. Let a learner be given a set of actions $i\in\A$ such that $|A|=K$. Let, the environment be such that each action has a probability distribution $D_i$ attached to it which is fixed throughout the time horizon $T$. The learning proceeds as follows:-

\begin{algorithm}[!th]
\caption{An online sequential game}
\label{alg:OSeqGame}
\begin{algorithmic}
\State {\bf Input:} Time horizon $T$, $K$ number of arms with unknown parameters of reward distribution
\State \For{ each timestep $t=1,2,\ldots, T$}
\State The environment chooses a reward $r_{i,t},\forall i\in\A$.
\State The learner chooses $m$ actions such that $m < K$, where $A$ is the set of arms and $|A|=K$.
\State The learner observes the reward $R_{m,t}=F\left( r_{i,t}\right)$.
\State \EndFor
\end{algorithmic}
\end{algorithm}

%Let $G(V,E)$ denote a graph where $V$ denotes the set of nodes in the graph and $E$ denotes the set of edges of the graph. Let there be a single starting node, denoted by $\mathcal{s}\in V$ from where the learner must start and try to reach the destination node denoted by $\mathcal{d}\in V$. Each edge has a delay associated with it which is unknown to the learner. This delay is an $i.i.d$ random variable from the distribution $D_{ij}$ associated with the edge $e_{ij}$ between the vertices $v_i$ and $v_j$. Whenever an edge is chosen the environment reveals to the learner  At every timestep the learner chooses a set of edges and receives some form of feedback from the environment. The goal of the learner is to find the path from $s$ to $d$ which has the minimum delay associated with it. 


\subsection{Full information feedback}
In full information feedback, when a learner selects $m$ actions then the environment reveals the rewards of all the actions $i\in \A$. Hence, in this form of feedback  the learner observes $R_{m,t} = \lbrace r_{i,t},\forall i\in\A\rbrace$.


\subsection{Partial information feedback}
In partial information feedback, when a learner selects $m$ actions then the environment reveals the rewards of only those $m$ actions for $m\in \A$. Hence, in this form of feedback  the learner observes $R_{m,t} = \lbrace r_{m,t},\forall m\in\A\rbrace$. This is also sometimes called the semi-bandit feedback.


\subsection{Bandit feedback}
In bandit feedback, when a learner selects $m$ actions then the environment reveals a cumulative reward of those $m$ actions for $m\in \A$. Hence, in this form of feedback  the learner observes $R_{m,t} = \sum_{q=1}^{m} r_{q,t}$. Note, that when $m=1$, then the learner observes the reward of only that action that it has chosen out of $K$ actions.


