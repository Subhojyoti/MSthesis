As defined in the previous section, an episode consists of a series of sequential interaction whereas agent transitions from one state to another based on some intrinsic dynamics of the environment while collecting the rewards and choosing actions based on some action-selecting strategy. The MAB model can be considered as a special case where there is a single looping state and the agent after taking an action and observing the reward transitions back to the same state. That single looping state consists of several finite number of actions which are called as arms.

    The name bandit originated from the concept of casino slot machine where there are levers which are called as arms and the learner can pull one lever and observes the reward associated with that arm which is sampled from a distribution associated with the specific arm. This game is repeated $T$ times and the goal of the learner is to maximize its profit. 

% Now, for a single-step interaction, i.e., when the episode terminates after a single transition, the problem is captured by the multi-armed bandit (MAB) model.