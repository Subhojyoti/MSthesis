As defined in the previous section, an episode consist of a series of sequential interaction where as agent transitions from one state to another based on some intrinsic dynamics of the environment while collecting the rewards and choosing actions based on some action-selecting strategy. Now, for a single-step interaction, i.e., when the episode terminates after a single transition, the problem is captured by the multi-armed bandit (MAB) model. Infact the MAB model can be considered as a single looping state when the agent after taking an action, and observing the reward transitions back to the same state. That single looping state consist of several finite number of actions which are called as arms.

	The name bandit originated from the concept of casino slot machine where there are levers which are called as arms and the learner can pull one lever and observes the reward associated with that arm which is sampled from a distribution associated with the specific arm. This game is repeated $T$ times and the goal of the learner is to maximize its profit. 
