\subsection{Lower bound in SMAB}	
	
	SMAB problems have been extensively studied in several earlier works such as \citet{thompson1933likelihood}, \citet{robbins1952some} and \citet{lai1985asymptotically}. Lai and Robbins in  \citet{lai1985asymptotically} established an asymptotic lower bound for the cumulative regret. It showed that for any consistent allocation strategy, we can have
\begin{align*}
\liminf_{T \to \infty}\frac{\E[R_{T}]}{\log T}\geq\sum_{\{i:r_{i}<r^{*}\}}\frac{(r^{*}-r_{i})}{D(Q_{i}||Q^{*})}
\end{align*}	
where $D(Q_{i}||Q^{*})$ is the Kullback-Leibler divergence between the reward densities $Q_{i}$ and $Q^{*}$, corresponding to arms with mean $r_{i}$ and $r^{*}$, respectively.

\subsection{The Upper Confidence bound approach}
	
	Over the years SMABs have seen several algorithms with strong regret guarantees. For further reference an interested reader can look into \citet{bubeck2012regret}. The upper confidence bound algorithms balance the exploration-exploitation dilemma by linking the uncertainty in estimate of an arm with the number of times an arm is pulled, and therefore ensuring sufficient exploration. 
	
\subsubsection{UCB1 Algorithm}	
	
	
	One of the earliest among these algorithms is UCB1 algorithm proposed in \citet{auer2002finite}. The algorithm is mentioned 
	
\begin{algorithm}[h!]
\caption{UCB1}
\label{alg:ucb1}
\begin{algorithmic}[1]
\State Pull each arm once
 \For{$t=K+1,..., T$}
\State Pull the arm such that $\argmax_{i\in A}\bigg\lbrace\hat{r}_{i} + \sqrt{\dfrac{2\log (t)}{n_i}}\bigg\rbrace$
\State $t:=t+1 $
 \EndFor
\end{algorithmic}
\end{algorithm}
	
	
	
	UCB1 has a gap-dependent regret upper bound of  $O\left(\frac{K\log T}{\Delta}\right)$, where $\Delta = \min_{i:\Delta_i>0} \Delta_i$. This result is asymptotically order-optimal for the class of distributions considered. But, the worst case gap-independent regret bound of UCB1 is found to be  $O \left(\sqrt{KT\log T}\right)$. 
	
\subsubsection{UCB-Improved Algorithm}		
	
\begin{algorithm}[!h]
\caption{UCB-Improved}
\label{alg:ucbi}
\begin{algorithmic}[1]
\State {\bf Input:} Time horizon $T$
\State {\bf Initialization:} Set $B_{0}:=A$ and $\epsilon_{0}:=1$.
\For{$m=0,1,..\big \lfloor \dfrac{1}{2}\log_{2} \dfrac{T}{e}\big\rfloor$}	
\State Pull each arm in $B_m$, $n_{m}=\bigg\lceil\dfrac{2\log{( T\epsilon_{m}^{2})}}{\tilde{\Delta}_{m}}\bigg\rceil$ number of times.
%so that the total  it has been pulled is
\ArmElim
\State For each $i \in B_{m}$, delete arm ${i}$ from $B_{m}$ if,
\begin{align*}
\hat{r}_{i} + \sqrt{\dfrac{\log{(T\epsilon_{m}^{2})}}{2 n_{m}}}  < \max_{{j}\in B_{m}}\bigg\lbrace\hat{r}_{j} -\sqrt{\dfrac{\log{( T\epsilon_{m}^{2})}}{2 n_{m}}} \bigg\rbrace
\end{align*}
\EndArmElim
%\ResParam
\State Set $\epsilon_{m+1}:=\dfrac{\epsilon_{m}}{2}$, Set $B_{m+1}:=B_{m}$
%\EndResParam
\State Stop if $|B_{m}|=1$ and pull ${i}\in B_{m}$ till $n$ is reached.
\EndFor
\end{algorithmic}
\end{algorithm}
	
	The UCB-Improved algorithm, proposed in \citet{auer2010ucb}, is a round-based variant of UCB1. An algorithm is \textit{round-based} if it pulls all the arms equal number of times in each round and then eliminates one or more arms that it deems  to be sub-optimal. UCB-Improved incurs a gap-dependent regret bound of $O\left(\frac{K\log (T\Delta^{2})}{\Delta}\right)$, which is better than that of UCB1. On the other hand, the worst case gap-independent regret bound of UCB-Improved is $O\left(\sqrt{KT\log K}\right)$.	
		
	
\subsubsection{MOSS Algorithm}	

\begin{algorithm}[!h]
\caption{MOSS}
\label{alg:moss}
\begin{algorithmic}[1]
\State Pull each arm once
 \For{$t=K+1,..., T$}
\State Pull the arm such that $\argmax_{i\in A}\bigg\lbrace\hat{r}_{i} + \sqrt{\dfrac{\max\lbrace 0,\log(\frac{T}{K s_i})\rbrace}{s_i}}\bigg\rbrace$
\State $t:=t+1 $
 \EndFor
\end{algorithmic}
\end{algorithm}
	
	In the later work of \citet{audibert2009minimax}, the authors propose the MOSS algorithm and showed that the worst case gap-independent regret bound of MOSS is $O\left( \sqrt{KT} \right)$ which improves upon UCB1 by a factor of order $\sqrt{\log T}$. However, the gap-dependent regret of MOSS is $O\left( \frac{K^{2}\log\left(T\Delta^{2}/K\right)}{\Delta}\right)$ and in certain regimes, this can be worse than even UCB1 (see \citet{audibert2009minimax,lattimore2015optimally}).
	
	Recently in \citet{lattimore2015optimally}, the authors showed that  the algorithm OCUCB achieves order-optimal gap-dependent regret bound of $O\left(\sum_{i=2}^{K}\frac{\log\left(T/H_i\right)}{\Delta_i}\right)$ where $H_i=\sum_{j=1}^{K}\min\left\lbrace \frac{1}{\Delta_i^2},\frac{1}{\Delta_j^2}\right\rbrace$, and a gap-independent regret bound of $O\left( \sqrt{KT}\right)$. This is the best known gap-dependent and gap-independent regret bounds in the stochastic MAB framework. However, unlike our proposed EUCBV algorithm, OCUCB does not take into account the variance of the arms; as a result, empirically  we find  that our algorithm outperforms OCUCB in all the environments considered. 

\begin{algorithm}[h!]
\caption{UCBV}
\label{alg:ucbv}
\begin{algorithmic}[1]
\State Pull each arm once
 \For{$t=K+1,..., T$}
\State Pull the arm such that $\max_{i\in A}\bigg\lbrace\hat{r}_{i} + \sqrt{\dfrac{2\hat{v}_i\log (t)}{s_i}} + \dfrac{3\log (t)}{3}\bigg\rbrace$
\State $t:=t+1 $
 \EndFor
\end{algorithmic}
\end{algorithm}


	In contrast to the above work, the UCBV \citep{audibert2009exploration} algorithm utilizes variance estimates to compute the confidence intervals for each arm. UCBV has a gap-dependent regret bound of $O\left(\frac{K\sigma_{\max}^{2}\log T}{\Delta}\right)$, where $\sigma_{\max}^{2}$ denotes the maximum variance among all the arms $i\in \A$. Its gap-independent regret bound can be inferred to be same as that of UCB1 i.e $O \left(\sqrt{KT\log T}\right)$. Empirically, \citet{audibert2009exploration} showed that UCBV outperforms UCB1 in several scenarios. 


\subsection{Bayesian Approach}

\begin{algorithm}[!h]
\caption{Bernoulli Thompson Sampling}
\label{alg:ts}
\begin{algorithmic}
\State {\bf Input:} Time horizon $T$; 
\State {\bf Initialization:} For each arm $i:=1$ to $K$ set $S_i =0$ and $F_i =0$
\State \For{$t=1,..,T$}
\State \For{$i=1,..,K$}
\State Sample $\theta_{i}(t)$ from the $Beta(S_i+1,F_i+1)$ distribution.
\EndFor
\State Play the arm $i(t):=\argmax_i\theta_i(t)$ and observe reward $X_{i,t}$.
\If{$X_{i,t}=1$}
$S_i (t) = S_i (t) + 1$
\Else{$F_i (t) = F_i (t) + 1$}
\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

	
	Another notable design principle which has recently gained a lot of popularity is the Thompson Sampling (TS) algorithm (\citep{thompson1933likelihood}, \citep{agrawal2011analysis})  and  Bayes-UCB (BU) algorithm \citep{kaufmann2012bayesian}. % which employs the Bayesian approach in solving the MAB problem.
The TS algorithm maintains a posterior reward distribution for each arm; at each round, the algorithm samples values from these distribution and the arm corresponding to the highest sample value is chosen. Although TS is found to perform extremely well when the reward distributions are Bernoulli, it is established that with Gaussian priors the worst case regret can be as bad as $\Omega \left( \sqrt{KT\log T}\right)$ \citep{lattimore2015optimally}. The BU algorithm is an extension of the TS algorithm that takes quartile deviations into consideration while choosing arms.

\subsection{Information Theoretic approach}
	
	The final design principle we state is the information theoretic approach of DMED  \citep{honda2010asymptotically} and KLUCB \citep{garivier2011kl} algorithms. The algorithm KLUCB uses Kullbeck-Leibler divergence to compute the upper confidence bound for the arms. KLUCB is stable for a short horizon and is known to reach the \citet{lai1985asymptotically} lower bound in the special case of Bernoulli distribution. However, \citet{garivier2011kl} showed that KLUCB, MOSS and UCB1 algorithms are  empirically outperformed by UCBV in the exponential distribution as they do not take the variance of the arms into consideration. 