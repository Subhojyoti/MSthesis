With the formulation of SMAB stated in Algorithm \ref{alg:SMAB}, the learner seeks to identify the optimal arm as quickly as possible to maximize its rewards. In the pursuit of this, the learner faces the task of balancing exploitation and exploration. In other words, should the learner pull the arm which currently has the best-known estimates (exploit) or explores arms more thoroughly to ensure that a correct decision is being made. This is termed as the \textit{exploration-exploitation dilemma}, one of the fundamental challenges of reinforcement learning as discussed in chapter \ref{chap:intro}.


    The objective of the learner in the SMAB setting is to maximize his rewards or in other words, to minimize the cumulative regret, which is defined as follows:
\begin{align*}
R_{T}=r^{*}T - \sum_{i=1}^{K} r_{i}z_{i}(T),
\end{align*}
where $T$ is the number of timesteps, and  $z_{i}(T)$ is the number of times the algorithm has chosen arm $i$ up to timestep $T$.
The expected regret of an algorithm after $T$ timesteps can be written as,
\begin{align*}
\E[R_{T}]= \sum_{i=1}^{K} \E[z_i (T)] \Delta_i,
\end{align*}
where $\Delta_{i}=r^{*}-r_{i}$ is the gap between the means of the optimal arm and the $i$-th arm. In the theoretical analysis of each algorithm, we try to obtain bounds on this cumulative regret. These bounds can be both asymptotic or for a finite horizon. Again, these regret bounds can be either gap-dependent or gap-independent bounds. 

\begin{enumerate}
\item\textbf{Asymptotic regret bounds:} These type of regret bounds are valid for a large horizon $T$ tending to infinity. In other words, if the guarantees of these bounds to be held true then an infinite number of samples needs to be collected.

\item\textbf{Finite horizon regret bounds:} These type of regret bounds are valid for a finite horizon when a limited number of samples are allowed to be collected. Note, that the knowledge of horizon may or may not be known to the learner.

\item\textbf{Gap-Dependent regret bounds:} In gap-dependent or problem dependent regret bounds the regret is obtained as a measure of the gap $\Delta_{i}=r^{*}-r_{i}$ for an arm $i\in\A$ along with the time horizon and number of arms. It is so called because the regret bound depends explicitly on the means of the arms considered for that environment along with the stated assumptions on the distribution.

\item\textbf{Gap-Independent regret bounds:} In gap-independent regret bound the regret does not contain the gaps and is stated explicitly in terms of the number of arms and the horizon. This is because the regret depends only on the distributional assumption, but not on the means of the arms considered. In fact, gap-independent regret bounds point to something more general and informative. These type of bounds actually give us the maximum possible regret such that no matter what is the policy, there will be an environment on which the policy achieves almost the same regret as the gap-independent regret upper bound. This leads to the notion of minimax regret.

\item\textbf{Minimax regret bounds:} For a finite horizon $T$, $K$ number of arms, for all set of possible policies $\pi_{T,K}$ over $T$ and $K$ and all possible environment class $\mathcal{E}$ the minimax regret is given by,
\begin{align*}
R_T(\mathcal{E})=\inf_{\pi\in\pi_{T,K}}\sup_{E\in\mathcal{E}}R_T(\pi,E).
\end{align*}

Hence, this value is independent of any specific choice of a policy $\pi$ but only depends on $T$, $K$ and $\mathcal{E}$ where the dependence on $K$ is hidden in $\mathcal{E}$.
\end{enumerate}
